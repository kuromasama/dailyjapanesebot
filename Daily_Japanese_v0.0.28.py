import google.generativeai as genai
import requests
import os
import json
import random
import re
from datetime import datetime, timedelta, timezone
import time
import math

# ================= ç’°å¢ƒè®Šæ•¸ =================
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
TG_BOT_TOKEN = os.getenv("TG_BOT_TOKEN")
TG_CHAT_ID = os.getenv("TG_CHAT_ID")

# æª”æ¡ˆè¨­å®š
VOCAB_FILE = "vocab.json"
USER_DATA_FILE = "user_data.json"
LOG_FILE = "TG_MSG.log"
MODEL_NAME = 'models/gemini-2.5-flash' 

# N2 è¡åˆºè¨­å®š (åŠå¹´ = 180å¤©)
SPRINT_DURATION_DAYS = 180
TARGET_DIFFICULTY = 4.0
START_DIFFICULTY = 1.0 # è¨­å®š N5 ç‚ºèµ·é»

# å…¨å±€æ—¥èªŒç·©è¡å€
LOG_BUFFER = []
TW_TZ = timezone(timedelta(hours=8))

# å®‰å…¨è¨­å®š
SAFETY_SETTINGS = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]

# ================= æª”æ¡ˆå­˜å–å·¥å…· =================

def load_json(filename, default_content):
    if os.path.exists(filename):
        try:
            with open(filename, "r", encoding="utf-8") as f:
                data = json.load(f)
                # ç¢ºä¿èˆŠæœ‰ vocab æ ¼å¼ç›¸å®¹
                if filename == VOCAB_FILE and "words" in data:
                    for w in data["words"]:
                        if "type" not in w: w["type"] = "word"
                        if "count" not in w: w["count"] = 1
                
                if isinstance(data, dict) and isinstance(default_content, dict):
                    for k, v in default_content.items():
                        if k not in data: data[k] = v
                return data
        except: return default_content
    return default_content

def save_json(filename, data):
    if filename == USER_DATA_FILE and "translation_log" in data:
        if len(data["translation_log"]) > 100:
            data["translation_log"] = data["translation_log"][-100:]
            
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def log_to_buffer(role, message):
    timestamp = datetime.now(TW_TZ).strftime('%H:%M:%S')
    LOG_BUFFER.append(f"[{timestamp}] {role}: {message}")

def send_telegram(message):
    if not message: return
    
    # è¨˜éŒ„åˆ° Log Buffer (å®Œæ•´è¨˜éŒ„)
    log_to_buffer("ğŸ¤– Bot", message)

    if not TG_BOT_TOKEN: print(f"[æ¨¡æ“¬ç™¼é€] {message[:50]}..."); return

    clean_msg = message.replace("**", "").replace("##", "").replace("__", "")
    clean_msg = re.sub(r'<br\s*/?>', '\n', clean_msg)
    
    try:
        # ğŸ”¥ ä¿®å¾©ï¼šç§»é™¤ Markdown èªæ³•ï¼Œæ¢å¾©æ­£å¸¸ URL
        requests.post(f"https://api.telegram.org/bot{TG_BOT_TOKEN}/sendMessage", json={"chat_id": TG_CHAT_ID, "text": clean_msg
        })
    except Exception as e: print(f"TG ç™¼é€å¤±æ•—: {e}")

def normalize_text(text):
    if not text: return ""
    return text.strip().replace("ã€€", " ").lower()

# ================= Log å¯«å…¥åŠŸèƒ½ =================

def write_log_file(user_data):
    stats = user_data["stats"]
    current_difficulty = float(stats.get("current_difficulty", 2.0))
    days_passed, _, sprint_msg = get_sprint_status(user_data)
    
    diff_cn_jp = float(stats.get("difficulty_cn_jp", current_difficulty))
    diff_jp_cn = float(stats.get("difficulty_jp_cn", current_difficulty))
    
    header = f"""# ğŸ“Š N2 è¡åˆºè¨ˆç•« - å­¸ç¿’ç‹€æ…‹å„€è¡¨æ¿
Last Updated: {datetime.now(TW_TZ).strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“ˆ ç›®å‰èƒ½åŠ›å€¼ (é›™è»Œåˆ¶)
- **ä¸­ç¿»æ—¥ç­‰ç´š (è¼¸å‡º)**: Lv {diff_cn_jp:.2f}
- **æ—¥ç¿»ä¸­ç­‰ç´š (è¼¸å…¥)**: Lv {diff_jp_cn:.2f}
- **è¡åˆºé€²åº¦**: Day {days_passed} / {SPRINT_DURATION_DAYS}
- **ç‹€æ…‹è©•èª**: {sprint_msg}
- **é€£çºŒç™»å…¥**: {stats.get('streak_days', 0)} å¤©

## âš”ï¸ è¨“ç·´æ•¸æ“š
- **åŸ·è¡Œå›æ•¸**: {stats.get('execution_count', 0)} å›
- **ç´¯ç©ç­”é¡Œ**: {stats.get('daily_answers_count', 0) + stats.get('bonus_answers_count', 0)} (ä»Šæ—¥è¨ˆæ•¸)
- **ä¸Šæ¬¡æ›´æ–° ID**: {stats.get('last_update_id', 0)}

---
> ä»¥ä¸‹ç‚ºå°è©±ç´€éŒ„ (ç”±æ–°åˆ°èˆŠæ’åº)

"""
    old_logs = ""
    separator = "=== ğŸ“œ HISTORY LOGS START ===\n"
    
    if os.path.exists(LOG_FILE):
        try:
            with open(LOG_FILE, "r", encoding="utf-8") as f:
                content = f.read()
                if separator in content:
                    old_logs = content.split(separator)[1]
                else:
                    old_logs = content
        except: pass

    new_log_entry = ""
    if LOG_BUFFER:
        new_log_entry = f"\n### ğŸ—“ï¸ {datetime.now(TW_TZ).strftime('%Y-%m-%d Execution')}\n"
        new_log_entry += "\n".join(LOG_BUFFER) + "\n"
        new_log_entry += "\n----------------------------------------\n"

    full_content = header + separator + new_log_entry + old_logs

    try:
        with open(LOG_FILE, "w", encoding="utf-8") as f:
            f.write(full_content)
        print("âœ… Log file updated successfully.")
    except Exception as e:
        print(f"âš ï¸ Failed to write log file: {e}")

# ================= è¼”åŠ©åŠŸèƒ½ =================

def get_sprint_status(user_data):
    stats = user_data["stats"]
    # è¡åˆºç‹€æ…‹ä»¥ã€Œä¸­ç¿»æ—¥ã€é›£åº¦ç‚ºä¸»è¦åŸºæº–
    current_difficulty = float(stats.get("difficulty_cn_jp", stats.get("current_difficulty", 1.0)))

    if current_difficulty >= TARGET_DIFFICULTY:
        return 0, 0, "infinity"

    if "sprint_start_date" not in stats:
        stats["sprint_start_date"] = str(datetime.now(TW_TZ).date())
        return 0, 0, "start"

    start_date = datetime.strptime(stats["sprint_start_date"], "%Y-%m-%d").date()
    today = datetime.now(TW_TZ).date()
    days_passed = (today - start_date).days
    
    if days_passed <= 0: days_passed = 1

    progress_ratio = min(1.0, days_passed / SPRINT_DURATION_DAYS)
    
    # ç°¡å–®è¨ˆç®—è½å¾Œèˆ‡å¦
    days_total = SPRINT_DURATION_DAYS
    expected_diff_now = START_DIFFICULTY + (days_passed / days_total) * (TARGET_DIFFICULTY - START_DIFFICULTY)
    
    diff_val = current_difficulty - expected_diff_now
    daily_growth = (TARGET_DIFFICULTY - START_DIFFICULTY) / days_total
    days_gap = int(diff_val / daily_growth)

    # æ¢å¾© v0.0.14 ç”Ÿå‹•çš„èªæ°£
    status_msg = ""
    if days_gap >= 5:
        status_msg = f"ğŸ”¥ è¶…å‰é€²åº¦ï¼šä½ æ¯”é æœŸå¿«äº† {days_gap} å¤©ï¼ä¿æŒé€™ç¨®ç¥é€Ÿï¼ŒN2 æ ¹æœ¬æ˜¯å›Šä¸­ä¹‹ç‰©ï¼"
    elif days_gap <= -5:
        status_msg = f"âš ï¸ è½å¾Œè­¦å ±ï¼šä½ å·²ç¶“è½å¾Œè¨ˆç•« {abs(days_gap)} å¤©äº†ï¼è·é›¢ N2 è¶Šä¾†è¶Šé å›‰ï¼Ÿçš®ç¹ƒç·Šä¸€é»ï¼"
    else:
        status_msg = f"âœ… é€²åº¦æ­£å¸¸ï¼šç©©æ­¥é‚å‘ N2 ä¸­ï¼Œè«‹ç¹¼çºŒä¿æŒé€™ä»½ç¯€å¥ã€‚"

    return days_passed, expected_diff_now, status_msg

# ================= AI æ ¸å¿ƒåŠŸèƒ½ =================

def assess_user_level(history_logs, specific_request=None):
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(MODEL_NAME)
    
    print("ğŸ§  AI æ­£åœ¨é€²è¡Œå…¨ç›¤èƒ½åŠ›è©•ä¼°...")
    log_to_buffer("ğŸ§  AI", "åŸ·è¡Œèƒ½åŠ›è©•ä¼° ([LV])")

    manual_level_map = {"n5": 1.0, "n4": 2.0, "n3": 3.0, "n2": 4.0, "n1": 5.0}
    
    if specific_request:
        req_lower = specific_request.lower().replace(" ", "")
        for key, val in manual_level_map.items():
            if key in req_lower:
                return val, f"æ”¶åˆ°æŒ‡ä»¤ï¼Œæ•™ç·´å·²å°‡é›£åº¦å¼·åˆ¶è¨­å®šç‚º {key.upper()} (Lv{val})ã€‚"
        
        match = re.search(r"(\d+(\.\d+)?)", specific_request)
        if match:
            val = float(match.group(1))
            return val, f"æ”¶åˆ°æŒ‡ä»¤ï¼Œé›£åº¦è¨­å®šç‚º Lv{val}ã€‚"

    history_text = "\n".join(history_logs[-50:])
    
    # ä½¿ç”¨è®Šæ•¸æ›¿æ›é¿å… Markdown æˆªæ–·
    json_marker = "```"
    
    # ğŸ”¥ å¼·åŒ– Promptï¼šè¦æ±‚ç†ç”±ä¹Ÿå¿…é ˆæœ‰æ•™ç·´èªæ°£
    prompt = f"""
    ä½ æ˜¯æ—¥æ–‡ N2 æ–¯å·´é”æ•™ç·´ã€‚ä½¿ç”¨è€…è¦æ±‚é‡æ–°è©•ä¼°å¥¹çš„æ—¥æ–‡ç­‰ç´šã€‚
    
    ã€ä½¿ç”¨è€…çš„æ­·å²ç¿»è­¯ç´€éŒ„ã€‘
    {history_text}
    
    è«‹æ ¹æ“šé€™äº›ç´€éŒ„ï¼Œå®¢è§€ä¸”åš´æ ¼åœ°åˆ¤æ–·å¥¹çš„æ—¥æ–‡ç¨‹åº¦ã€‚
    ç›®å‰çš„é›£åº¦é‡è¡¨å¦‚ä¸‹ï¼š
    - Lv 1.0: N5 (å–®å­—ç‚ºä¸»)
    - Lv 2.0: N4 (ç°¡å–®å¥å­)
    - Lv 3.0: N3 (æ—¥å¸¸æœƒè©±)
    - Lv 4.0: N2 (å•†æ¥­/æ–°èå…¥é–€)
    - Lv 5.0: N1 (é«˜éšç¶œåˆ)
    - Lv 6.0+: æ¯èªè€…/å°ˆæ¥­é ˜åŸŸ
    
    è«‹çµ¦å‡ºä¸€å€‹ **ç²¾ç¢ºçš„æµ®é»æ•¸ (ä¾‹å¦‚ 2.4 æˆ– 3.8)**ã€‚
    **åˆ¤æ–·é‡é»ï¼šä¸è¦åªçœ‹å–®å­—é‡ï¼Œè«‹é‡é»è©•ä¼°å¥¹çš„ã€ŒåŠ©è©ä½¿ç”¨æ­£ç¢ºç‡ã€ã€ã€Œå‹•è©è®ŠåŒ–çš„ç†Ÿç·´åº¦ã€ä»¥åŠã€Œå¥å‹çš„è±å¯Œåº¦ã€ã€‚**
    
    ã€è¼¸å‡ºæ ¼å¼ (JSON)ã€‘
    è«‹åªå›å‚³ JSONï¼Œä¸è¦æœ‰ markdown æ¨™è¨˜ï¼š
    {{ 
      "new_difficulty": 2.5, 
      "reason": "ä½ çš„å–®å­—é‡ä¸éŒ¯ï¼Œä½†åŠ©è©é‚„æ˜¯å¸¸éŒ¯ï¼Œå»ºè­°å¾ N3 å‰åŠæ®µé–‹å§‹ç£¨ç·´ã€‚(è«‹ç”¨æ•™ç·´èªæ°£æ’°å¯«æ­¤ç†ç”±ï¼Œåš´å²ä½†ä¸­è‚¯)" 
    }}
    """
    
    try:
        response = model.generate_content(prompt, safety_settings=SAFETY_SETTINGS)
        clean_text = response.text.replace("```json", "").replace("```", "").strip()
        result = json.loads(clean_text)
        return float(result["new_difficulty"]), result["reason"]
    except Exception as e:
        print(f"è©•ä¼°å¤±æ•—: {e}")
        return None, "ç„¡æ³•è©•ä¼°ï¼Œç¶­æŒåŸé›£åº¦ã€‚"

def handle_custom_request(user_text, current_stats):
    """
    [RE] åŠŸèƒ½ï¼šè™•ç†ä½¿ç”¨è€…çš„å®¢è£½åŒ–è«‹æ±‚ (èª¿æ•´é›£åº¦æˆ–æŒ‡å®šå‡ºé¡Œæ–¹å‘)
    """
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(MODEL_NAME)
    
    print("ğŸ§  AI æ­£åœ¨è™•ç†å®¢è£½åŒ–è«‹æ±‚...")
    log_to_buffer("ğŸ§  AI", f"è™•ç†è«‹æ±‚: {user_text}")

    diff_cn_jp = current_stats.get('difficulty_cn_jp', 1.0)
    diff_jp_cn = current_stats.get('difficulty_jp_cn', 1.0)

    # ä½¿ç”¨è®Šæ•¸æ›¿æ›é¿å… Markdown æˆªæ–·
    json_marker = "```"

    # ğŸ”¥ å¼·åŒ– Promptï¼šè¦æ±‚å‰µæ„èˆ‡å¤šæ¨£æ€§
    prompt = f"""
    ä½ æ˜¯æ—¥æ–‡ N2 æ–¯å·´é”æ•™ç·´ã€‚ä½¿ç”¨è€…é€é [RE] æŒ‡ä»¤å‚³é€äº†å®¢è£½åŒ–è«‹æ±‚ï¼š
    ã€Œ{user_text}ã€

    ç›®å‰ä½¿ç”¨è€…ç‹€æ…‹ï¼š
    - ä¸­ç¿»æ—¥ç­‰ç´š: Lv {diff_cn_jp}
    - æ—¥ç¿»ä¸­ç­‰ç´š: Lv {diff_jp_cn}

    è«‹åŸ·è¡Œä»¥ä¸‹ä»»å‹™ï¼š
    1. **æ•™ç·´å›æ‡‰**ï¼šç”¨æ–¯å·´é”é¢¨æ ¼ï¼ˆåš´å²ä½†é—œå¿ƒï¼Œå¹½é»˜æ¯’èˆŒï¼‰å›æ‡‰ä½¿ç”¨è€…ã€‚
       - **âš ï¸ å‰µæ„è¦æ±‚**ï¼šè«‹ä¸è¦ä½¿ç”¨å›ºå®šçš„æ¨¡æ¿ï¼ˆä¾‹å¦‚ä¸è¦æ¯æ¬¡éƒ½èªªã€Œçœ‹åœ¨ä½ å¿«å“­çš„ä»½ä¸Šã€ï¼‰ã€‚è«‹æ ¹æ“šä½¿ç”¨è€…çš„å…·é«”è«‹æ±‚å…§å®¹å’Œèªæ°£ï¼Œå³èˆˆç™¼æ®ã€‚
       - å¦‚æœæ˜¯**æ±‚é¥’**ï¼ˆè¦ºå¾—å¤ªé›£ï¼‰ï¼šå¯ä»¥å˜²è«·ä»–çš„æ„å¿—åŠ›ï¼Œæˆ–æ˜¯ç”¨æ¿€å°‡æ³•ï¼Œæœ€å¾Œå†å‹‰å¼·ç­”æ‡‰ã€‚
       - å¦‚æœæ˜¯**æ±‚çŸ¥**ï¼ˆæƒ³å­¸ç‰¹å®šæ–‡æ³•/å–®å­—ï¼‰ï¼šèª‡çä»–çš„é‡å¿ƒï¼Œä¸¦æ‰¿è«¾åœ¨ä¸‹æ¬¡å‡ºé¡Œæ™‚åŠ å…¥ã€‚
       - å¦‚æœæ˜¯**é–’èŠ**ï¼šç”¨æ•™ç·´èº«ä»½å›æ‡‰ï¼Œæé†’ä»–å»ç·´ç¿’ã€‚
    
    2. **ç³»çµ±æŒ‡ä»¤ (JSON)**ï¼šåœ¨å›æ‡‰æœ€å¾Œé™„ä¸Š JSONï¼Œå‘Šè¨´ç³»çµ±å¦‚ä½•èª¿æ•´ã€‚
       æ ¼å¼ï¼š
       {json_marker}json
       {{
         "actions": {{
            "adjust_difficulty": -0.2,  
            "quiz_instruction": "å‡ºé¡Œæ™‚è«‹åŠ å…¥'å› ç‚ºã€å„˜ç®¡'ç­‰è½‰æŠ˜è©çš„ç·´ç¿’ã€‚" 
         }}
       }}
       {json_marker}
       - **adjust_difficulty**: æµ®é»æ•¸ã€‚æ­£æ•¸è®Šé›£ï¼Œè² æ•¸è®Šç°¡å–®ã€‚0 å‰‡ä¸è®Šã€‚è‹¥ä½¿ç”¨è€…è¦ºå¾—å¤ªé›£ï¼Œå»ºè­° -0.2 ~ -0.5ã€‚
       - **quiz_instruction**: å­—ä¸²ã€‚çµ¦ã€Œä¸‹ä¸€æ¬¡æ¯æ—¥æ¸¬é©—ç”Ÿæˆã€çš„é¡å¤–æŒ‡ä»¤ã€‚å¦‚æœä½¿ç”¨è€…è¦æ±‚ç‰¹å®šå…§å®¹ï¼Œè«‹å°‡å…¶æ¿ƒç¸®åœ¨æ­¤ã€‚è‹¥ç„¡å‰‡ç•™ç©ºå­—ä¸²ã€‚
    
    ã€æ ¼å¼è¦æ±‚ã€‘
    è«‹ç›´æ¥è¼¸å‡ºæ•™ç·´çš„å›æ‡‰æ–‡å­—ï¼ŒJSON å€å¡Šæ”¾åœ¨æœ€å¾Œé¢ã€‚
    """

    try:
        response = model.generate_content(prompt, safety_settings=SAFETY_SETTINGS)
        return response.text if response.text else "âš ï¸ AI å›æ‡‰å¤±æ•—"
    except Exception as e:
        return f"âš ï¸ AI è™•ç†éŒ¯èª¤: {e}"

def ai_correction(user_text, translation_history, progress_status):
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(MODEL_NAME)
    
    print(f"ğŸ¤– AI æ­£åœ¨æ‰¹æ”¹ (é€²åº¦ {progress_status})...")
    history_str = "\n".join(translation_history[-10:]) if translation_history else "(å°šç„¡æ­·å²ç´€éŒ„)"

    # ä½¿ç”¨è®Šæ•¸æ›¿æ›é¿å… Markdown æˆªæ–·
    json_marker = "```"
    
    # ğŸ”¥ æ–¯å·´é”æ•™ç·´ Prompt - v0.0.25 (ä¿ç•™èªæ„ŸåŠ åˆ†èˆ‡éŒ¯èª¤æ‡²ç½°åˆ†é›¢) + v0.0.27 (å‰µæ„é–å®š)
    prompt = f"""
    ä½¿ç”¨è€…æ­£åœ¨å›ç­” N2 ç¿»è­¯æ¸¬é©—ï¼Œé€™æ˜¯å¥¹å‰›å‰›å‚³ä¾†çš„å…§å®¹ï¼ˆå¯èƒ½åŒ…å«å¤šå¥ï¼‰ï¼š
    ã€Œ{user_text}ã€
    
    ã€æ­·å²ç´€éŒ„ã€‘
    {history_str}
    
    ã€ç•¶å‰ç­”é¡Œé€²åº¦ã€‘
    {progress_status}
    
    è«‹æ‰®æ¼”æ—¥æ–‡æ•™æˆèˆ‡æ–¯å·´é”æ•™ç·´ï¼Œå®Œæˆä»¥ä¸‹ä»»å‹™ï¼š
    
    1. **ğŸ” åˆ¤æ–·è¼¸å…¥èªè¨€ (é—œéµ)**ï¼š
       - **è‹¥ä½¿ç”¨è€…è¼¸å…¥æ—¥æ–‡**ï¼šé€™ä»£è¡¨å¥¹åœ¨åšã€Œä¸­ç¿»æ—¥ã€ã€‚è«‹**æ¥µåº¦åš´æ ¼**åœ°æ‰¹æ”¹ã€‚
         - **é‡é»æª¢æŸ¥**ï¼šåŠ©è© (ã¦ã«ã‚’ã¯) æ˜¯å¦ç²¾æº–ï¼Ÿå‹•è©è®ŠåŒ– (æ´»ç”¨) æ˜¯å¦æ­£ç¢ºï¼Ÿæ™‚æ…‹æ˜¯å¦ç¬¦åˆèªå¢ƒï¼Ÿæœ‰æ²’æœ‰ä¸­å¼æ—¥æ–‡ (Chinglish) çš„å•é¡Œï¼Ÿ
       - **è‹¥ä½¿ç”¨è€…è¼¸å…¥ä¸­æ–‡**ï¼šé€™ä»£è¡¨å¥¹åœ¨åšã€Œæ—¥ç¿»ä¸­ã€ã€‚è«‹**ä¸è¦**æŠŠå®ƒç¿»è­¯å›æ—¥æ–‡ï¼è«‹è¦–ç‚ºå¥¹æ˜¯å°çš„ï¼Œä¸¦è©•ä¼°å¥¹çš„ä¸­æ–‡ç¿»è­¯æ˜¯å¦é€šé †ã€å„ªç¾ (ä¿¡é”é›…)ã€‚
    
    2. **ğŸ¯ æ·±åº¦æ‰¹æ”¹ (é€å¥æª¢è¨) - æ ¸å¿ƒåƒ¹å€¼è§€é‡å¡‘**ï¼š
       - ä½¿ç”¨è€…çš„ç›®æ¨™æ˜¯ **ã€Œåƒæ—¥æœ¬äººä¸€æ¨£èªªè©± (Natural Native Japanese)ã€**ï¼Œè€Œä¸åƒ…æ˜¯æ•™ç§‘æ›¸æ—¥æ–‡ã€‚
       - **âŒ éŒ¯èª¤ (Mistake)**ï¼šæ–‡æ³•éŒ¯èª¤ã€æ™‚æ…‹éŒ¯èª¤ã€ç”¨è©æ„æ€å®Œå…¨éŒ¯èª¤ã€‚ -> **å¿…é ˆåˆ—å…¥ JSON ä¸¦æ‰£åˆ†**ã€‚
       - **â• èªæ„Ÿ/é¢¨æ ¼ (Nuance)**ï¼šå£èªã€ä¿šèªã€éæ­£å¼ç”¨æ³• (å¦‚ã€Œã‚³ãƒ¼ãƒ’ãƒ¼å±‹ã€ã€ã€Œç¾å‘³ã„ã€ã€ã€Œï½ã¡ã‚ƒã£ãŸã€)ã€‚
         - **çµ•å°ç¦æ­¢**æŠŠé“åœ°çš„å£èªè¦–ç‚ºéŒ¯èª¤ï¼
         - å¦‚æœæ–‡æ³•æ­£ç¢ºä½†é¢¨æ ¼éš¨æ„ï¼Œè«‹çµ¦äºˆ **ã€Œâ• èªæ„Ÿæé†’ã€**ï¼Œè§£é‡‹ï¼šã€Œé€™æ˜¯å¾ˆé“åœ°çš„å£èªï¼Œé©åˆæœ‹å‹é–“ä½¿ç”¨ï¼Œè‹¥åœ¨å•†å‹™å ´åˆå»ºè­°æ”¹ç”¨...ã€ã€‚
         - **è«‹çµ¦äºˆé€™é¡é“åœ°ç”¨æ³•é«˜åº¦è©•åƒ¹ (åŠ åˆ†)**ï¼Œå› ç‚ºé€™ä»£è¡¨ä½¿ç”¨è€…è„«é›¢äº†æ­»æ¿çš„æ•™ç§‘æ›¸ã€‚
    
    3. **âœ¨ ä¸‰ç¨®å¤šæ¨£åŒ–è¡¨é” (å¿…é ˆåŒ…å«)**ï¼š
       - é‡å°æ¯ä¸€å¥ï¼Œå±•ç¤ºä¸åŒæƒ…å¢ƒçš„ç”¨æ³•ï¼š
         1. **ğŸ‘” æ­£å¼/æ›¸é¢** (é©åˆå ±å‘Šæˆ–é•·è¼©)
         2. **ğŸ» å£èª/æœ‹å‹** (é“åœ°ç”Ÿæ´»æ„Ÿ)
         3. **ğŸ”„ æ›å¥è©±èªª** (ä½¿ç”¨**å®Œå…¨ä¸åŒçš„å¥å‹æˆ–å–®å­—**è¡¨é”åŒä¸€å€‹æ„æ€ï¼Œè¨“ç·´è©å½™é‡èˆ‡éˆæ´»åº¦)
       - è‹¥è¼¸å…¥æ˜¯ä¸­æ–‡ï¼šæä¾›ä¸‰ç¨®ä¸åŒé¢¨æ ¼çš„ä¸­æ–‡è­¯æ³• (ä¾‹å¦‚ï¼šç›´è­¯ã€æ„è­¯ã€æ–‡è¨€/æˆèªä¿®é£¾)ã€‚
    
    4. **ğŸ‘¹ æ–¯å·´é”å³æ™‚ç£ä¿ƒ**ï¼š
       - **æƒ…æ³ A (å¿…ä¿®)**ï¼šé€²åº¦è½å¾Œè¦å¹½é»˜å˜²è«·ï¼Œå¿«å®Œæˆè¦é¼“å‹µã€‚
       - **æƒ…æ³ B (Bonus)**ï¼šçµ•å°ç¦æ­¢ç½µäººï¼Œè«‹çµ¦äºˆé«˜åº¦è‚¯å®šã€‚
         
    5. **ğŸš¨ ã€ç³»çµ±æŒ‡ä»¤ï¼šéŒ¯èª¤æ”¶éŒ„ã€‘(éå¸¸é‡è¦)**ï¼š
       - è«‹åœ¨å›æ‡‰çš„**æœ€å¾Œé¢**ï¼Œé™„ä¸Šä¸€å€‹ JSON å€å¡Šã€‚
       - **é—œéµè¦å‰‡**ï¼šåªæœ‰ **ã€ŒçœŸæ­£çš„æ–‡æ³•/æ„æ€éŒ¯èª¤ã€** æ‰èƒ½æ”¾é€² `mistakes`ï¼
       - **èªæ„Ÿå»ºè­°ã€å£èªç”¨æ³•ã€æ›´å„ªé›…çš„èªªæ³•** -> **çµ•å°ä¸è¦** æ”¾é€² `mistakes`ï¼Œå¯«åœ¨æ–‡å­—è©•èªè£¡å°±å¥½ã€‚
       
    6. **ğŸ“Š ã€é€å¥è©•åˆ†èˆ‡æ•™ç·´çŸ­è©•ã€‘(v22 æ ¸å¿ƒå‡ç´š)**ï¼š
       - ä½ ä¸å†åªæ˜¯çµ¦ç¸½åˆ†ï¼Œè«‹é‡å°ä½¿ç”¨è€…çš„**æ¯ä¸€å€‹å›ç­”å¥**ï¼Œçµ¦äºˆç¨ç«‹çš„è©•åˆ†èˆ‡åæ‡‰ã€‚
       - åœ¨æ–‡å­—å›æ‡‰ä¸­ï¼Œè«‹ç”¨ä»¥ä¸‹æ ¼å¼åˆ—å‡ºæ¯å¥çš„è©•åƒ¹ï¼š
         ã€ŒQ1: 9.5åˆ† - (æ•™ç·´çŸ­è©•: å“‡å–”ï¼é€™å¥åŠ©è©ç”¨å¾—å¤ªç¥äº†ï¼Œç°¡ç›´æ˜¯æ—¥æœ¬äººæŠ•èƒï¼)ã€
         ã€ŒQ2: 4.0åˆ† - (æ•™ç·´çŸ­è©•: é–‰è‘—çœ¼ç›å¯«çš„å—ï¼Ÿæ™‚æ…‹å®Œå…¨éŒ¯äº†ï¼Œçµ¦æˆ‘é‡å¯«ï¼)ã€
         ã€ŒQ3: 0.0åˆ† - (æ•™ç·´çŸ­è©•: ç©ºç™½ï¼Ÿä½ æ˜¯è¢«å¤–æ˜Ÿäººç¶æ¶äº†å—ï¼Ÿé€™é¡Œä¸äºˆç½®è©•ï¼)ã€
       - **âš ï¸ å‰µæ„è¦æ±‚ï¼šä»¥ä¸Šæ‹¬è™Ÿå…§çš„çŸ­è©•åƒ…ç‚ºã€Œèªæ°£ç¯„ä¾‹ã€ï¼Œçµ•å°ç¦æ­¢ç…§æŠ„ï¼è«‹æ ¹æ“šä½¿ç”¨è€…å¯¦éš›çŠ¯çš„éŒ¯èª¤ï¼ˆå¦‚æ™‚æ…‹ã€æ•¬èªã€å–®å­—ï¼‰æˆ–æ˜¯ç²¾å½©ä¹‹è™•ï¼Œå³èˆˆå‰µä½œå‡ºã€Œç•¶ä¸‹æœ€è²¼åˆ‡ã€çš„æ¯’èˆŒæˆ–è®šç¾ã€‚è«‹å±•ç¾ä½ è±å¯Œçš„è©å½™é‡ï¼Œä¸è¦é‡è¤‡ã€‚**
       - **è©•åˆ†æ¨™æº–**ï¼š
         - **9.0~10.0 (ç¥ç´š)**ï¼šæ–‡æ³•å®Œç¾ï¼Œèªæ„Ÿé“åœ° (åŒ…å«é“åœ°å£èª)ï¼Œä½¿ç”¨äº†é€²éšèªå½™/æ›å¥è©±èªªã€‚
         - **7.0~8.9 (åˆæ ¼)**ï¼šæ­£ç¢ºç„¡èª¤ï¼Œä¸­è¦ä¸­çŸ©ã€‚
         - **6.0~6.9 (å‹‰å¼·)**ï¼šæœ‰å°éŒ¯ä½†ä¸å½±éŸ¿ç†è§£ã€‚
         - **< 6.0 (ä¸åŠæ ¼)**ï¼šæ–‡æ³•éŒ¯èª¤ï¼Œèªæ„ä¸æ¸…ã€‚
         - **0.0 (å·æ‡¶)**ï¼šç©ºç™½ã€äº‚ç¢¼ã€æ˜é¡¯æ”¾æ£„ä½œç­”ã€‚
    
    7. **JSON è¼¸å‡ºè¦æ±‚ (é™£åˆ—åŒ–)**ï¼š
       - è«‹åœ¨å›æ‡‰çš„**æœ€å¾Œé¢**ï¼Œé™„ä¸Šä¸€å€‹ JSON å€å¡Šï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
       {json_marker}json
       {{
         "mistakes": [
            {{ "term": "èª¤ç”¨è©", "type": "word", "meaning": "è©æ„" }}
         ],
         "assessments": [
            {{
                "input": "ä½¿ç”¨è€…è¼¸å…¥çš„ç¬¬ä¸€å¥",
                "type": "CN_TO_JP" æˆ– "JP_TO_CN", 
                "score": 9.5,
                "status": "ATTEMPTED" æˆ– "SKIPPED" 
            }},
            {{
                "input": "ä½¿ç”¨è€…è¼¸å…¥çš„ç¬¬äºŒå¥",
                "type": "CN_TO_JP", 
                "score": 0.0,
                "status": "SKIPPED" 
            }}
         ]
       }}
       {json_marker}
       - **status**: è‹¥è¼¸å…¥ç‚ºç©ºç™½ã€"ä¸çŸ¥é“"ã€"..." ç­‰æ˜é¡¯æœªä½œç­”ï¼Œæ¨™è¨˜ç‚º "SKIPPED"ã€‚å¦å‰‡ç‚º "ATTEMPTED"ã€‚
    
    ã€æ ¼å¼åš´æ ¼è¦æ±‚ã€‘
    1. **èªè¨€**ï¼šè§£èªªèˆ‡è©•èªè«‹å…¨ç¨‹ä½¿ç”¨ã€Œç¹é«”ä¸­æ–‡ã€(Traditional Chinese)ã€‚
    2. **æ’ç‰ˆ**ï¼š
       - **åš´ç¦** ä½¿ç”¨ Markdown æ¨™é¡Œ (å¦‚ # æˆ– ##)ã€‚
       - è«‹ä½¿ç”¨ Emoji (å¦‚ ğŸ“ˆ, ğŸ¯, âœ¨, ğŸ‘¹, ğŸ‘”, ğŸ», ğŸ”„) ä¾†å€éš”ã€‚
       - **åš´ç¦** ä½¿ç”¨ HTML æ¨™ç±¤ (å¦‚ <br>)ï¼Œè«‹ç›´æ¥æ›è¡Œã€‚
    """
    
    try:
        response = model.generate_content(prompt, safety_settings=SAFETY_SETTINGS)
        return response.text if response.text else "âš ï¸ AI æ‰¹æ”¹å¤±æ•—"
    except Exception as e:
        return f"âš ï¸ AI æ‰¹æ”¹éŒ¯èª¤: {e}"

# ================= é‚è¼¯æ ¸å¿ƒ =================

def process_data():
    print("ğŸ“¥ é–‹å§‹è™•ç†è³‡æ–™...")
    log_to_buffer("âš™ï¸ Sys", "Checking for updates...")
    
    # é è¨­çš„å®Œæ•´ä½¿ç”¨è€…è³‡æ–™çµæ§‹
    default_user_data = {
        "stats": {
            "last_active": "2000-01-01", 
            "streak_days": 0,
            "execution_count": 0,
            "last_quiz_date": "2000-01-01",
            "last_quiz_questions_count": 0,
            "daily_answers_count": 0,
            "bonus_answers_count": 0,
            "yesterday_main_score": 0,
            "yesterday_bonus_score": 0,
            "last_update_id": 0,
            "current_difficulty": START_DIFFICULTY, 
            "difficulty_cn_jp": START_DIFFICULTY,
            "difficulty_jp_cn": START_DIFFICULTY,
            "sprint_start_date": str(datetime.now(TW_TZ).date()),
            "next_quiz_instruction": "" 
        },
        "pending_answers": "",
        "translation_log": []
    }
    
    vocab_data = load_json(VOCAB_FILE, {"words": []})
    user_data = load_json(USER_DATA_FILE, default_user_data)
    
    stats = user_data["stats"]
    stats["current_difficulty"] = float(stats.get("current_difficulty", START_DIFFICULTY))
    stats["difficulty_cn_jp"] = float(stats.get("difficulty_cn_jp", stats["current_difficulty"]))
    stats["difficulty_jp_cn"] = float(stats.get("difficulty_jp_cn", stats["current_difficulty"]))
    if "next_quiz_instruction" not in stats: stats["next_quiz_instruction"] = ""

    for key in ["daily_answers_count", "bonus_answers_count", "yesterday_main_score", 
                "yesterday_bonus_score", "execution_count", "streak_days", "last_update_id"]:
        if key not in stats: stats[key] = 0

    url = f"https://api.telegram.org/bot{TG_BOT_TOKEN}/getUpdates"
    
    try:
        # ğŸ”¥ ä¿®å¾©ï¼šç§»é™¤ Markdown èªæ³•ï¼Œæ¢å¾©æ­£å¸¸ URL
        response = requests.get(url).json()
        if "result" not in response: 
            log_to_buffer("âš™ï¸ Sys", "No 'result' in TG response.")
            return vocab_data, user_data
        
        is_updated = False
        updates_log = []
        correction_msgs = []
        
        today_str = str(datetime.now(TW_TZ).date())
        today_answers_detected = 0
        pending_correction_texts = []
        
        last_processed_id = user_data["stats"]["last_update_id"]
        is_fresh_start = (last_processed_id == 0)
        max_id_in_this_run = last_processed_id
        
        found_count = 0
        for item in response["result"]:
            current_update_id = item["update_id"]
            if current_update_id <= last_processed_id: continue
            if current_update_id > max_id_in_this_run: max_id_in_this_run = current_update_id

            message_obj = item.get("message")
            if not message_obj: 
                continue

            if str(message_obj["chat"]["id"]) != str(TG_CHAT_ID): continue
            
            text = message_obj.get("text", "").strip()
            if not text: continue
            
            # ğŸ”¥ ä¿®æ­£ï¼šå„ªå…ˆæ­£è¦åŒ–æ‹¬è™Ÿï¼Œç¢ºä¿å…¨å½¢ ï¼»CHï¼½ ä¹Ÿèƒ½è¢«è­˜åˆ¥
            text = text.replace("ï¼»", "[").replace("ï¼½", "]")
            
            found_count += 1
            msg_time = datetime.fromtimestamp(message_obj["date"], TW_TZ).strftime('%H:%M:%S')
            log_to_buffer("ğŸ‘¤ User", f"{text} (ID: {current_update_id})")

            # [LV] æŒ‡ä»¤ (æ›´åè‡ª [CH])
            if text.upper().startswith("[LV]"):
                if is_fresh_start: continue
                specific_req = text[4:].strip()
                new_diff, reason = assess_user_level(user_data["translation_log"], specific_req)
                if new_diff is not None:
                    user_data["stats"]["current_difficulty"] = new_diff
                    user_data["stats"]["difficulty_cn_jp"] = new_diff
                    user_data["stats"]["difficulty_jp_cn"] = new_diff
                    updates_log.append(f"ğŸ§  AI è©•ç´šå®Œæˆï¼šèª¿æ•´è‡³ Lv{new_diff}ã€‚\nğŸ’¬ ç†ç”±ï¼š{reason}")
                    is_updated = True
                continue
            
            # [RE] å®¢è£½åŒ–è«‹æ±‚
            if text.upper().startswith("[RE]"):
                if is_fresh_start: continue
                request_content = text[4:].strip()
                
                # å‘¼å«å®¢è£½åŒ–è™•ç†å‡½å¼
                raw_response = handle_custom_request(request_content, user_data["stats"])
                
                # è§£æ AI å›å‚³çš„ JSON æŒ‡ä»¤
                final_reply = raw_response
                try:
                    json_match = re.search(r"```json\s*(\{.*?\})\s*```", raw_response, re.DOTALL)
                    if json_match:
                        json_str = json_match.group(1)
                        action_data = json.loads(json_str)
                        final_reply = raw_response.replace(json_match.group(0), "").strip()
                        
                        if "actions" in action_data:
                            actions = action_data["actions"]
                            # 1. èª¿æ•´é›£åº¦
                            adj_val = float(actions.get("adjust_difficulty", 0.0))
                            if adj_val != 0.0:
                                user_data["stats"]["difficulty_cn_jp"] = max(1.0, user_data["stats"]["difficulty_cn_jp"] + adj_val)
                                user_data["stats"]["difficulty_jp_cn"] = max(1.0, user_data["stats"]["difficulty_jp_cn"] + adj_val)
                                log_to_buffer("âš™ï¸ Adjust", f"Difficulty adjusted by {adj_val}")
                            
                            # 2. è¨­å®šä¸‹æ¬¡å‡ºé¡ŒæŒ‡ä»¤
                            quiz_instr = actions.get("quiz_instruction", "")
                            if quiz_instr:
                                user_data["stats"]["next_quiz_instruction"] = quiz_instr
                                log_to_buffer("âš™ï¸ Instruct", f"Next quiz instruction set: {quiz_instr}")
                                is_updated = True
                except Exception as e:
                    log_to_buffer("âš ï¸ Err", f"RE parsing failed: {e}")

                updates_log.append(f"ğŸ—£ï¸ æ•™ç·´å›æ‡‰ï¼š\n{final_reply}")
                is_updated = True
                continue

            # Case A: JSON åŒ¯å…¥
            if text.startswith("["):
                try:
                    imported = json.loads(text)
                    if isinstance(imported, list):
                        added = 0
                        for word in imported:
                            if "kanji" not in word: continue
                            kanji = word.get("kanji")
                            if not any(normalize_text(w["kanji"]) == normalize_text(kanji) for w in vocab_data["words"]):
                                vocab_data["words"].append({
                                    "kanji": kanji, 
                                    "kana": word.get("kana", ""),
                                    "meaning": word.get("meaning", ""),
                                    "type": word.get("type", "word"),
                                    "count": 1, "added_date": today_str
                                })
                                added += 1
                                is_updated = True
                        updates_log.append(f"ğŸ“‚ åŒ¯å…¥ {added} å€‹æ–°é …ç›®")
                except: pass
                continue

            # Case B: å­˜å–®å­—/æ–‡æ³•
            match = re.search(r"^([^/\s]+)(?:[ \u3000]+|/)([^/\s]+)(?:[ \u3000]+|/)(.+)$", text)
            if match:
                if is_fresh_start: continue
                term, kana_or_info, meaning = match.groups()
                if not term.lower().startswith("part") and len(text) < 50: 
                    found = False
                    for word in vocab_data["words"]:
                        if normalize_text(word["kanji"]) == normalize_text(term):
                            word["count"] += 1 
                            updates_log.append(f"ğŸ”„ å¼·åŒ–è¨˜æ†¶ï¼š{term}")
                            found = True
                            is_updated = True
                            break
                    if not found:
                        item_type = "grammar" if ("~" in term or "..." in term) else "word"
                        vocab_data["words"].append({
                            "kanji": term, "kana": kana_or_info, "meaning": meaning, 
                            "type": item_type,
                            "count": 1, "added_date": today_str
                        })
                        updates_log.append(f"âœ… æ”¶éŒ„ ({item_type})ï¼š{term}")
                        is_updated = True
                    continue

            # Case C: ç¿»è­¯/ä½œæ¥­
            if not text.startswith("/"):
                if is_fresh_start: continue
                lines_count = len([l for l in text.split('\n') if len(l.strip()) > 1])
                lines_count = max(1, lines_count)
                today_answers_detected += lines_count
                
                pending_correction_texts.append(text)
                user_data["translation_log"].append(f"{today_str}: {text[:100]}")
                is_updated = True

        if found_count == 0:
            log_to_buffer("âš™ï¸ Sys", "No new user messages found.")
        else:
             log_to_buffer("âš™ï¸ Sys", f"Processed {found_count} new messages.")

        # === è¨ˆç®—è¨ˆåˆ† ===
        if today_answers_detected > 0:
            current_main = user_data["stats"]["daily_answers_count"]
            main_quota = 10 
            remaining_quota = max(0, main_quota - current_main)
            fill_main = min(today_answers_detected, remaining_quota)
            user_data["stats"]["daily_answers_count"] += fill_main
            spill_to_bonus = today_answers_detected - fill_main
            if spill_to_bonus > 0:
                user_data["stats"]["bonus_answers_count"] += spill_to_bonus
            is_updated = True

        # === æ‰¹æ”¹è™•ç† ===
        if not is_fresh_start and pending_correction_texts:
            combined_text = "\n\n".join(pending_correction_texts)
            history_context = user_data["translation_log"][:-len(pending_correction_texts)]
            
            main_count = user_data["stats"]["daily_answers_count"]
            bonus_count = user_data["stats"]["bonus_answers_count"]
            
            if bonus_count > 0:
                progress_str = f"ç‹€æ…‹ï¼šBonus æŒ‘æˆ°ä¸­ (å·²å®Œæˆ {bonus_count} é¡Œ Bonus)"
            else:
                progress_str = f"ç‹€æ…‹ï¼šæ¯æ—¥å¿…ä¿®é€²è¡Œä¸­ ({main_count}/10 é¡Œ)"

            raw_result = ai_correction(combined_text, history_context, progress_str)
            
            final_msg_text = raw_result
            mistaken_terms = []
            
            # ç•¶æ—¥/ç•¶æ¬¡å¹³å‡åˆ†æ•¸è¨ˆç®—
            total_score_sum = 0.0
            total_score_count = 0

            # è§£æéŒ¯èª¤èˆ‡è©•ä¼° JSON
            try:
                json_match = re.search(r"```json\s*(\{.*?\})\s*```", raw_result, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                    parsed_data = json.loads(json_str)
                    
                    final_msg_text = raw_result.replace(json_match.group(0), "").strip()
                    log_to_buffer("âš™ï¸ AI Feed", f"JSON: {json_str}")
                    
                    # 1. è™•ç†éŒ¯èª¤ (Mistakes)
                    if "mistakes" in parsed_data:
                        mistake_log_list = []
                        for m in parsed_data["mistakes"]:
                            term = m.get("term", "")
                            m_type = m.get("type", "word")
                            meaning = m.get("meaning", "AI ä¿®æ­£")
                            
                            if term:
                                mistakes_found_in_vocab = False
                                for w in vocab_data["words"]:
                                    if normalize_text(w["kanji"]) == normalize_text(term):
                                        w["count"] = w.get("count", 1) + 2 # ç­”éŒ¯æ‡²ç½°
                                        w["type"] = m_type 
                                        mistaken_terms.append(normalize_text(term))
                                        mistakes_found_in_vocab = True
                                        mistake_log_list.append(f"âš ï¸ å¼±é»æ¨™è¨˜ (æ¬Šé‡+2): {term}")
                                        break
                                if not mistakes_found_in_vocab:
                                    vocab_data["words"].append({
                                        "kanji": term, "kana": "", "meaning": meaning,
                                        "type": m_type, "count": 5, "added_date": today_str
                                    })
                                    mistaken_terms.append(normalize_text(term))
                                    mistake_log_list.append(f"ğŸ†• å¼±é»æ”¶éŒ„ (æ¬Šé‡=5): {term}")
                        if mistake_log_list:
                             updates_log.extend(mistake_log_list)
                             is_updated = True

                    # 2. é€å¥è©•åˆ†èˆ‡é›™è»Œé›£åº¦èª¿æ•´ (Assessment List)
                    if "assessments" in parsed_data and isinstance(parsed_data["assessments"], list):
                        for item in parsed_data["assessments"]:
                            status = item.get("status", "ATTEMPTED")
                            score = float(item.get("score", 0.0))
                            q_type = item.get("type", "")
                            target_key = "difficulty_cn_jp" if q_type == "CN_TO_JP" else "difficulty_jp_cn"
                            
                            # ğŸš¨ é˜²å·æ‡¶æ ¸å¿ƒï¼šåªæœ‰ ATTEMPTED æ‰æœƒèª¿æ•´é›£åº¦èˆ‡è¨ˆç®—ç¸½åˆ†
                            if status == "ATTEMPTED":
                                total_score_sum += score
                                total_score_count += 1
                                
                                if q_type in ["CN_TO_JP", "JP_TO_CN"]:
                                    # é›£åº¦å³æ™‚èª¿æ•´é‚è¼¯
                                    if score >= 9.0: # ç¥ç´š (+0.1)
                                        user_data["stats"][target_key] = min(8.0, user_data["stats"][target_key] + 0.1)
                                    elif score >= 7.0: # åˆæ ¼ (+0.05)
                                        user_data["stats"][target_key] = min(8.0, user_data["stats"][target_key] + 0.05)
                                    elif score < 6.0: # ä¸åŠæ ¼ (-0.1)
                                        user_data["stats"][target_key] = max(1.0, user_data["stats"][target_key] - 0.1)

            except Exception as e:
                log_to_buffer("âš ï¸ Err", f"JSON parsing failed: {e}")

            # 3. æ¬Šé‡å›èª¿æ©Ÿåˆ¶ (çå‹µç­”å°)
            text_for_search = normalize_text(combined_text)
            for w in vocab_data["words"]:
                if normalize_text(w["kanji"]) in text_for_search:
                    if normalize_text(w["kanji"]) not in mistaken_terms:
                        if w.get("count", 1) > 1:
                            w["count"] = max(1, w["count"] - 2) # ç­”å°çå‹µ

            # 4. ç”Ÿæˆç¸½è©•åˆ†å­—ä¸²
            score_summary = ""
            if total_score_count > 0:
                avg_score = total_score_sum / total_score_count
                rank = "C"
                if avg_score >= 9.0: rank = "SSS"
                elif avg_score >= 8.0: rank = "S"
                elif avg_score >= 7.0: rank = "A"
                elif avg_score >= 6.0: rank = "B"
                score_summary = f"\n\nğŸ“Š **æœ¬æ¬¡å¹³å‡æˆ°åŠ›ï¼š{avg_score:.1f} / 10.0 (Rank {rank})**"

            title_text = f"ğŸ“ **ä½œæ¥­æ‰¹æ”¹ (å…± {len(pending_correction_texts)} å‰‡)ï¼š**"
            correction_msgs.append(f"{title_text}\n{final_msg_text}{score_summary}")

        if max_id_in_this_run > user_data["stats"]["last_update_id"]:
            user_data["stats"]["last_update_id"] = max_id_in_this_run
            is_updated = True

        if user_data["stats"]["last_active"] != today_str:
            if today_answers_detected > 0 or is_updated:
                 yesterday = str((datetime.now(TW_TZ) - timedelta(days=1)).date())
                 if user_data["stats"]["last_active"] == yesterday:
                     user_data["stats"]["streak_days"] += 1
                 else:
                     user_data["stats"]["streak_days"] = 1
                 user_data["stats"]["last_active"] = today_str
                 is_updated = True

        if updates_log: send_telegram("\n".join(set(updates_log)))
        for msg in correction_msgs:
            send_telegram(msg)
            time.sleep(1)

        return vocab_data, user_data

    except Exception as e:
        print(f"Error: {e}")
        log_to_buffer("âš ï¸ Critical", f"Process data error: {e}")
        return load_json(VOCAB_FILE, {}), load_json(USER_DATA_FILE, default_user_data)

# ================= æ¯æ—¥ç‰¹è¨“ç”Ÿæˆ =================

def get_difficulty_description(level_float):
    level_int = int(level_float)
    descriptions = {
        1: "Lv1 (æ–°æ‰‹)ï¼šN5 åŸºç¤ï¼Œå°ˆæ³¨æ–¼å–®å­—è¨˜æ†¶ã€‚",
        2: "Lv2 (åˆç´š)ï¼šN4 æ–‡æ³•ï¼Œç°¡å–®è¤‡åˆå¥ã€‚",
        3: "Lv3 (ä¸­ç´š)ï¼šN3 æ—¥å¸¸æ‡‰ç”¨ï¼Œæ¨™æº–å°è©±ã€‚",
        4: "Lv4 (é«˜ç´š)ï¼šN2 å•†æ¥­/æ–°èå…¥é–€ï¼Œé•·é›£å¥ã€‚",
        5: "Lv5 (é­”é¬¼)ï¼šN1 é«˜éšç¶œåˆï¼Œè€ƒé©—æ¥µé™ã€‚",
        6: "Lv6 (æ¯èªè€…)ï¼šå°ˆæ¥­é ˜åŸŸã€æŠ€è¡“æ–‡ä»¶ã€è‰±æ¾€èªå½™ã€‚",
        7: "Lv7 (æ–‡å­¸)ï¼šå¤æ–‡ã€å“²å­¸ã€è©©è©é¢¨æ ¼ã€‚",
        8: "Lv8 (ç¥)ï¼šAI èªç‚ºäººé¡ç„¡æ³•é”åˆ°çš„å¢ƒç•Œã€‚"
    }
    base_desc = descriptions.get(level_int, f"Lv{level_int} (è¶…è¶Šæ¥µé™)")
    next_desc = descriptions.get(level_int + 1, f"Lv{level_int+1} (æœªçŸ¥)")
    return base_desc, next_desc

def run_daily_quiz(vocab, user):
    if not vocab.get("words"):
        send_telegram("ğŸ“­ å–®å­—åº«ç©ºçš„ï¼è«‹å‚³é€å–®å­—æˆ–åŒ¯å…¥ JSONã€‚")
        return user
    
    # è™•ç†ä¸Šæ¬¡è©³è§£
    pending_answers = user.get("pending_answers", "")
    if pending_answers:
        send_telegram(f"ğŸ—ï¸ **å‰æ¬¡æ¸¬é©—è©³è§£**\n\n{pending_answers}")
        time.sleep(3)
        user["pending_answers"] = ""
    
    today_str = str(datetime.now(TW_TZ).date())
    is_new_day = (user["stats"]["last_quiz_date"] != today_str)

    # === é¸è©é‚è¼¯ï¼šå¼±é»å„ªå…ˆ ===
    all_words = vocab["words"]
    sorted_words = sorted(all_words, key=lambda x: x.get("count", 1), reverse=True)
    
    weak_candidates = sorted_words[:10]
    normal_candidates = sorted_words[10:] if len(sorted_words) > 10 else []
    
    selected_weaks = weak_candidates[:3] if len(weak_candidates) >= 3 else weak_candidates
    needed_normal = 10 - len(selected_weaks)
    
    selected_normals = []
    if normal_candidates:
        weights = [w.get("count", 1) for w in normal_candidates]
        selected_normals = random.choices(normal_candidates, weights=weights, k=needed_normal)
    elif len(selected_weaks) < 10:
         selected_normals = random.choices(weak_candidates, k=10-len(selected_weaks))

    quiz_words = selected_weaks + selected_normals
    random.shuffle(quiz_words) 

    # ğŸ”¥ v0.0.28 ä¿®æ­£ï¼šå–®å­—åˆ—è¡¨å›æ»¾ç‚ºç°¡æ½”æ ¼å¼ (æ—¥æ–‡ + ä¸­æ–‡)ï¼Œé¿å… AI æ··æ·†
    word_list_str = "\n".join([f"{w['kanji']} ({w['meaning']})" for w in quiz_words])

    must_test_str = ", ".join([w['kanji'] for w in selected_weaks])

    # è®€å–é›™è»Œé›£åº¦
    diff_cn_jp = float(user["stats"].get("difficulty_cn_jp", 1.0))
    diff_jp_cn = float(user["stats"].get("difficulty_jp_cn", 1.0))
    
    days_passed, expected_diff, sprint_msg = get_sprint_status(user)
    is_infinite_mode = (sprint_msg == "infinity")

    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(MODEL_NAME)

    # ================= Scenario A: æ–°çš„ä¸€å¤© (æ¯æ—¥å¿…ä¿®) =================
    if is_new_day:
        user["stats"]["yesterday_main_score"] = user["stats"]["daily_answers_count"]
        user["stats"]["yesterday_bonus_score"] = user["stats"]["bonus_answers_count"]
        user["stats"]["daily_answers_count"] = 0
        user["stats"]["bonus_answers_count"] = 0
        user["stats"]["execution_count"] += 1
        exec_count = user["stats"]["execution_count"]
        streak_days = user["stats"]["streak_days"]
        main_score = user["stats"]["yesterday_main_score"]
        bonus_score = user["stats"]["yesterday_bonus_score"]
        
        emotion_prompt = ""
        difficulty_adjustment_msg = "" 

        answer_rate = main_score / 10
        if user["stats"]["last_quiz_date"] == "2000-01-01":
             emotion_prompt = f"""
             é€™æ˜¯ä½ ç¬¬ä¸€æ¬¡èˆ‡ä½¿ç”¨è€…è¦‹é¢ (Day 1)ã€‚
             **è«‹æ³¨æ„ï¼šè«‹å…¨ç¨‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ (Traditional Chinese)ã€‚**
             è«‹ç”¨å……æ»¿æ´»åŠ›ã€å°ˆæ¥­ä¸”æœŸå¾…çš„èªæ°£æ‰“æ‹›å‘¼ã€‚
             è‡ªæˆ‘ä»‹ç´¹ä½ æ˜¯ã€ŒN2 æ–¯å·´é” AI æ•™ç·´ã€ï¼Œä¸¦èªªæ˜æœªä¾†çš„è¨“ç·´æ¨¡å¼ï¼š
             ã€Œæ¯å¤©ä¸­åˆæˆ‘æœƒå‡ºé¡Œï¼Œéš”å¤©ä¸­åˆæˆ‘æœƒæª¢è¨æ˜¨å¤©çš„ä½œæ¥­ä¸¦å‡ºæ–°é¡Œç›®ã€‚ã€
             è«‹çµ¦äºˆä½¿ç”¨è€…æ»¿æ»¿çš„ä¿¡å¿ƒï¼
             """
        else:
            if answer_rate >= 0.8:
                difficulty_adjustment_msg = "ğŸ”¥ ä½ çš„è¡¨ç¾ç›¸ç•¶ç©©å®šï¼Œæ•™ç·´æˆ‘çœ‹åœ¨çœ¼è£¡ï¼"
                if bonus_score > 0:
                    emotion_prompt = f"æ˜¨æ—¥è¡¨ç¾ï¼šå¿…ä¿® {main_score}/10ï¼ŒBonus {bonus_score}ã€‚ç‹€æ…‹ï¼šç¥ä¸€èˆ¬çš„è‡ªå¾‹ï¼è«‹ç”¨æ¥µåº¦å´‡æ‹œèªæ°£èª‡çï¼ä¸¦æåˆ°ã€Œ{difficulty_adjustment_msg}ã€ã€‚"
                else:
                    emotion_prompt = f"æ˜¨æ—¥è¡¨ç¾ï¼šå¿…ä¿® {main_score}/10ã€‚ç‹€æ…‹ï¼šå„ªç§€ã€‚çµ¦äºˆé«˜åº¦è‚¯å®šã€‚ä¸¦æåˆ°ã€Œ{difficulty_adjustment_msg}ã€ã€‚"
            elif answer_rate >= 0.4:
                if not is_infinite_mode and diff_cn_jp < expected_diff:
                    emotion_prompt = f"æ˜¨æ—¥è¡¨ç¾ï¼šæ™®é€šã€‚é›–ç„¶æ²’é™ç´šï¼Œä½†æˆ‘å€‘è½å¾Œé€²åº¦äº†ï¼è«‹ç¨å¾®åš´è‚…ä¸€é»æé†’å¥¹åŠ å¿«è…³æ­¥ï¼šã€ç¾åœ¨ä¸æ˜¯ä¼‘æ¯çš„æ™‚å€™ï¼Œå·²ç¶“è½å¾Œè¨ˆç•«äº†ï¼ã€"
                else:
                    emotion_prompt = f"æ˜¨æ—¥è¡¨ç¾ï¼šå¿…ä¿® {main_score}/10ã€‚ç‹€æ…‹ï¼šå°šå¯ã€‚ç¹¼çºŒä¿æŒã€‚"
            else:
                sprint_warn = f"ç‰¹åˆ¥æ³¨æ„ï¼šè«‹å¼•ç”¨ã€Œè¡åˆºç‹€æ…‹æ•¸æ“š ({sprint_msg})ã€ä¾†è­¦å‘Šå¥¹ (ä¾‹å¦‚ï¼šæˆ‘å€‘å·²ç¶“è½å¾Œ X å¤©äº†ï¼Œé€™æ™‚å€™ç¡è¦ºå°å¾—èµ·ä½ çš„ N2 å ±åè²»å—ï¼Ÿ)ã€‚" if not is_infinite_mode else "è«‹æé†’å¥¹ï¼šç„¡é™ä¹‹è·¯ä¸é€²å‰‡é€€ï¼Œä¸è¦é¬†æ‡ˆäº†ï¼"
                emotion_prompt = f"""
                æ˜¨æ—¥è¡¨ç¾ï¼šå¿…ä¿® {main_score}/10ã€‚ç‹€æ…‹ï¼šå·æ‡¶ï¼
                è«‹é–‹å•Ÿã€å¹½é»˜æƒ…å‹’æ¨¡å¼ ğŸ˜ˆã€‘ã€‚
                {sprint_warn}
                **è«‹å…¨ç¨‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚**
                """
        
        print(f"ğŸ¤– ç”Ÿæˆæ¯æ—¥å¿…ä¿® - CN->JP Lv{diff_cn_jp:.1f}, JP->CN Lv{diff_jp_cn:.1f}...")
        
        desc_cn_jp, _ = get_difficulty_description(diff_cn_jp)
        desc_jp_cn, _ = get_difficulty_description(diff_jp_cn)
        
        sprint_info = "ç„¡é™æŒ‘æˆ°æ¨¡å¼" if is_infinite_mode else f"è¡åˆº Day {days_passed}/{SPRINT_DURATION_DAYS} ({sprint_msg})"

        # è®€å–ä¸¦é‡ç½®ä½¿ç”¨è€…å®¢è£½åŒ–æŒ‡ä»¤
        custom_instr_text = user["stats"].get("next_quiz_instruction", "")
        custom_block = f"ã€âš ï¸ ç‰¹åˆ¥å‡ºé¡ŒæŒ‡ä»¤ (ä¾†è‡ªä½¿ç”¨è€…è«‹æ±‚)ã€‘\n{custom_instr_text}\nè«‹å‹™å¿…åœ¨å‡ºé¡Œæ™‚èå…¥ä¸Šè¿°è¦æ±‚ã€‚" if custom_instr_text else ""
        if custom_instr_text:
             user["stats"]["next_quiz_instruction"] = "" # ç”¨å®Œå³ä¸Ÿ

        # ä½¿ç”¨è®Šæ•¸æ›¿æ›é¿å… Markdown æˆªæ–·
        json_marker = "```"

        prompt = f"""
        ä½ æ˜¯æ—¥æ–‡ N2 è¡åˆºç­æ•™ç·´ã€‚
        {sprint_info}
        
        **ğŸ¯ ä»Šæ—¥é›™è»Œé›£åº¦ç›®æ¨™ï¼š**
        - **ä¸­ç¿»æ—¥ (7é¡Œ)**: Lv {diff_cn_jp:.1f} ({desc_cn_jp})
        - **æ—¥ç¿»ä¸­ (3é¡Œ)**: Lv {diff_jp_cn:.1f} ({desc_jp_cn})
        
        ã€æƒ…ç·’èˆ‡é–‹å ´ã€‘
        {emotion_prompt}
        è«‹åœ¨é–‹å ´ç™½ä¸­æ˜ç¢ºæåˆ°ï¼šã€Œé€™æ˜¯æˆ‘å€‘çš„ç¬¬ {exec_count} æ¬¡ç‰¹è¨“ (Day {streak_days})ï¼ã€ã€‚
        ä¸¦æ ¹æ“šç›®å‰çš„é€²åº¦ç‹€æ…‹ (è½å¾Œã€è¶…å‰æˆ–ç„¡é™æŒ‘æˆ°)å±•ç¾å‡ºå°æ‡‰çš„æ•™ç·´æ…‹åº¦ã€‚
        **è«‹ä¸è¦æ¯æ¬¡éƒ½èªªä¸€æ¨£çš„è©±ã€‚è«‹æ ¹æ“šä»Šå¤©çš„æ—¥æœŸã€å¤©æ°£ï¼ˆå‡è¨­ï¼‰ã€æˆ–æ˜¯éš¨æ©Ÿçš„æ–¯å·´é”å“²å­¸ï¼Œè®ŠåŒ–ä½ çš„é–‹å ´ç™½ã€‚è®“ä½¿ç”¨è€…è¦ºå¾—ä½ æ˜¯æ´»ç”Ÿç”Ÿçš„æ•™ç·´ï¼Œè€Œä¸æ˜¯éŒ„éŸ³æ©Ÿã€‚**
        
        ã€ä»Šæ—¥å–®å­—åº« (å«å¼±é» ğŸ”¥)ã€‘
        {word_list_str}
        
        {custom_block}
        
        ã€å‡ºé¡Œçµæ§‹è¦æ±‚ (éå¸¸é‡è¦)ã€‘
        è«‹è£½ä½œ **10 é¡Œ** ç¿»è­¯æ¸¬é©—ï¼š
        1. **ä¸­ç¿»æ—¥ (7é¡Œ)**ï¼š
           - **é›£åº¦ç­‰ç´šï¼šLv {diff_cn_jp:.1f}** (è«‹ä¾ç…§æ­¤é›£åº¦è¨­è¨ˆå¥å­çµæ§‹)
           - **å¿…é ˆåŒ…å«é€™ 2 å€‹å¼±é»è©/æ–‡æ³•**ï¼š{must_test_str} (è«‹è¨­è¨ˆèƒ½ç·´ç¿’åˆ°é€™äº›è©çš„å¥å­)
           - å¦å¤– 5 é¡Œéš¨æ©Ÿå¾å–®å­—åº«é¸ã€‚
        2. **æ—¥ç¿»ä¸­ (3é¡Œ)**ï¼š
           - **é›£åº¦ç­‰ç´šï¼šLv {diff_jp_cn:.1f}** (å¯ä»¥æ¯”ä¸­ç¿»æ—¥æ›´é›£ï¼Œä½¿ç”¨æ›´é€²éšçš„é–±è®€æ¸¬é©—å¥å‹)
           - **å¿…é ˆåŒ…å« 1 å€‹å¼±é»è©/æ–‡æ³•** (å¾ä¸Šè¿°å¼±é»åˆ—è¡¨ä¸­é¸ä¸€å€‹ä¸åŒçš„)ã€‚
           - å¦å¤– 2 é¡Œéš¨æ©Ÿã€‚
           
        **æ³¨æ„ï¼šè‹¥æ˜¯æ¨™è¨˜ (æ–‡æ³•) çš„é …ç›®ï¼Œè«‹å‹™å¿…è¨­è¨ˆå‡ºèƒ½å±•ç¾è©²æ–‡æ³•æ¥çºŒèˆ‡ç”¨æ³•çš„å¥å­ã€‚**

        ã€ğŸš« å“è³ªç´…ç·š (çµ•å°ç¦æ­¢)ã€‘
        1. **åš´ç¦ã€Œä¸­å¼æ—¥æ–‡ (Chinglish)ã€**ï¼šåƒè€ƒç­”æ¡ˆçš„æ—¥æ–‡å¿…é ˆæ˜¯**å®Œå…¨é“åœ°çš„æ—¥æœ¬æ¯èªäººå£«ç”¨æ³•**ã€‚è«‹æª¢æŸ¥åŠ©è©èˆ‡æ­é…è©ï¼Œä¸è¦åªæ˜¯æŠŠä¸­æ–‡é‚è¼¯ç›´æ¥ç¿»æˆæ—¥æ–‡ã€‚
        2. **åš´ç¦ã€Œæ—¥å¼ä¸­æ–‡ (ç¿»è­¯è…”)ã€**ï¼šé¡Œç›®çš„ä¸­æ–‡å¿…é ˆæ˜¯**è‡ªç„¶æµæš¢çš„å°ç£ç¹é«”ä¸­æ–‡**ï¼Œä¸è¦å‡ºç¾ç”Ÿç¡¬çš„ç¿»è­¯å¥å‹ï¼ˆä¾‹å¦‚ä¸è¦å¯«ã€Œé—œæ–¼...é€™ä»¶äº‹ã€ï¼Œç›´æ¥å¯«ã€Œé—œæ–¼...ã€å³å¯ï¼‰ã€‚
        3. **é˜²æ­¢æ–‡æ³•é¡Œé¡¯ç¤ºéŒ¯èª¤**ï¼šåœ¨ã€Œä»Šæ—¥å–®å­—åº«ã€åˆ—è¡¨æˆ–ã€Œé¡Œç›®ã€ä¸­ï¼Œè‹¥é‡åˆ°æ–‡æ³•é …ç›®ï¼ˆä¾‹å¦‚ `~ã¦ã¯ã„ã‘ãªã„`ï¼‰ï¼Œ**è«‹å‹™å¿…é¡¯ç¤ºæ—¥æ–‡**ï¼Œçµ•å°ä¸è¦åªå¯«å‡ºä¸­æ–‡æ„æ€ï¼ˆå¦‚ `ç¦æ­¢åš...`ï¼‰ã€‚
        
        ã€è¼¸å‡ºæ ¼å¼è¦æ±‚ (åš´æ ¼éµå®ˆ)ã€‘
        1. **èªè¨€**ï¼š
           - é–‹å ´ç™½ã€å–®å­—é ç¿’ã€é¡Œç›®èªªæ˜ï¼š**å…¨ç¨‹ä½¿ç”¨ç¹é«”ä¸­æ–‡**ã€‚
           - é¡Œç›®æœ¬èº«ï¼šæ—¥æ–‡æˆ–ä¸­æ–‡ã€‚
        
        2. **æ’ç‰ˆ**ï¼š
           - **åš´ç¦** ä½¿ç”¨ Markdown æ¨™é¡Œ (å¦‚ # æˆ– ##)ã€‚
           - è«‹ä½¿ç”¨ Emoji (å¦‚ âš”ï¸, ğŸ“š, ğŸ“, ğŸ”¹) ä¾†å€éš”æ®µè½èˆ‡é …ç›®ã€‚
           - **åš´ç¦** ä½¿ç”¨ HTML æ¨™ç±¤ (å¦‚ <br>)ï¼Œè«‹ç›´æ¥æ›è¡Œã€‚
        
        3. **çµæ§‹**ï¼š
           - Part 1: é¡Œç›®å· (å«é–‹å ´ã€ç‹€æ…‹å›å ±ã€10é¡Œ)ã€‚**ä¸è¦**çµ¦ç­”æ¡ˆã€‚
           - åˆ†éš”ç·š: `|||SEPARATOR|||`
           - Part 2: è§£ç­”å· (å«åƒè€ƒç­”æ¡ˆèˆ‡è§£æ)ã€‚
        """
        
        try:
            response = model.generate_content(prompt, safety_settings=SAFETY_SETTINGS)
            if response.text and "|||SEPARATOR|||" in response.text:
                parts = response.text.split("|||SEPARATOR|||")
                send_telegram(parts[0].strip())
                user["pending_answers"] = parts[1].strip()
                user["stats"]["last_quiz_date"] = today_str
                user["stats"]["last_quiz_questions_count"] = 10
        except Exception as e:
            print(f"Error: {e}")
            send_telegram("âš ï¸ æ¸¬é©—ç”Ÿæˆå¤±æ•—")

    # ================= Scenario B: Bonus ç„¡é™æŒ‘æˆ° =================
    else:
        bonus_count = user["stats"]["bonus_answers_count"]
        avg_diff = (diff_cn_jp + diff_jp_cn) / 2
        bonus_difficulty = avg_diff + (bonus_count // 3) * 0.5 + 0.5 
        
        print(f"ğŸ¤– ç”Ÿæˆ Bonus - Lv{bonus_difficulty:.1f}...")
        
        base_level = int(bonus_difficulty)
        next_level = base_level + 1
        decimal_part = bonus_difficulty - base_level
        base_desc, next_desc = get_difficulty_description(bonus_difficulty)

        prompt = f"""
        ä½ æ˜¯æ—¥æ–‡ N2 æ–¯å·´é”æ•™ç·´ã€‚ä½¿ç”¨è€…ä»Šå¤©å·²ç¶“å®Œæˆæ¯æ—¥ä½œæ¥­ï¼Œä½†å¥¹**ä¸»å‹•**å†æ¬¡å›ä¾†åŸ·è¡Œç¨‹å¼ (æŒ‘æˆ° Bonus)ã€‚
        
        è«‹ç”¨ä¸€ç¨®**ã€Œå……æ»¿èª˜æƒ‘åŠ›èˆ‡æŒ‘æˆ°æ€§ã€**çš„èªæ°£é–‹å ´ã€‚
        **âš ï¸ å‰µæ„è¦æ±‚**ï¼šè«‹ä¸è¦æ¯æ¬¡éƒ½èªªä¸€æ¨£çš„è©±ã€‚è«‹æ ¹æ“šä»Šå¤©çš„æ—¥æœŸã€å¤©æ°£ï¼ˆå‡è¨­ï¼‰ã€æˆ–æ˜¯éš¨æ©Ÿçš„æ–¯å·´é”å“²å­¸ï¼Œè®ŠåŒ–ä½ çš„é–‹å ´ç™½ã€‚è®“ä½¿ç”¨è€…è¦ºå¾—ä½ æ˜¯æ´»ç”Ÿç”Ÿçš„æ•™ç·´ï¼Œè€Œä¸æ˜¯éŒ„éŸ³æ©Ÿã€‚
        é€™æ˜¯ä¸€ç¨®å°å¼·è€…çš„èªå¯ï¼ŒåŒæ™‚å¸¶æœ‰æŒ‘é‡æ„å‘³ï¼šã€Œåƒä¸€ä½é­”é¬¼æ•™ç·´çœ‹åˆ°å­¸å“¡ä¸»å‹•ç•™ä¸‹ä¾†åŠ ç·´æ™‚é‚£ç¨®ã€éœ²é½’ä¸€ç¬‘ã€çš„æ„Ÿè¦ºã€‚ğŸ˜ã€
        
        **ğŸ¯ Bonus é›£åº¦ç­‰ç´šï¼š{bonus_difficulty:.1f}**
        - Lv{base_level}: {base_desc} (ä½” {(1-decimal_part)*100:.0f}%)
        - Lv{next_level}: {next_desc} (ä½” {decimal_part*100:.0f}%)
        
        ã€ä»Šæ—¥å–®å­—åº« (å«å¼±é» ğŸ”¥)ã€‘
        {word_list_str}
        
        æä¾› **3 é¡Œ** ç¿»è­¯æŒ‘æˆ° (2ä¸­ç¿»æ—¥ï¼Œ1æ—¥ç¿»ä¸­)ã€‚
        **è«‹ç›¡é‡å„ªå…ˆä½¿ç”¨å–®å­—åº«ä¸­æ¨™è¨˜ç‚º ğŸ”¥ çš„å¼±é»é …ç›®ä¾†å‡ºé¡Œï¼ŒæŠ˜ç£¨ä½¿ç”¨è€…ï¼**

        ã€ğŸš« å“è³ªç´…ç·šã€‘
        **ç”Ÿæˆçš„æ—¥æ–‡è§£ç­”å¿…é ˆæ˜¯ã€Œçµ•å°é“åœ°ã€çš„æ—¥æ–‡ï¼Œåš´ç¦ä»»ä½•ã€Œä¸­å¼æ—¥æ–‡ã€çš„ç”Ÿç¡¬è¡¨é”ï¼è«‹ç”¨æ—¥æœ¬äººçš„æ€ç¶­ä¾†é€ å¥ã€‚**
        
        ã€è¼¸å‡ºæ ¼å¼è¦æ±‚ (åš´æ ¼éµå®ˆ)ã€‘
        1. **èªè¨€**ï¼š
           - é–‹å ´ç™½ã€é¡Œç›®èªªæ˜ï¼š**å…¨ç¨‹ä½¿ç”¨ç¹é«”ä¸­æ–‡**ã€‚
        
        2. **æ’ç‰ˆ**ï¼š
           - **åš´ç¦** ä½¿ç”¨ Markdown æ¨™é¡Œ (å¦‚ # æˆ– ##)ã€‚
           - è«‹ä½¿ç”¨ Emoji (å¦‚ ğŸ”¥, ğŸš€, ğŸ’¡, ğŸŒŸ) ä¾†å€éš”æ®µè½ã€‚
           - æ¨™é¡Œè«‹å¯«ï¼šâš”ï¸ **Bonus ç„¡é™æŒ‘æˆ° (Lv{bonus_difficulty:.1f})** âš”ï¸
           - **åš´ç¦** ä½¿ç”¨ HTML æ¨™ç±¤ (å¦‚ <br>)ï¼Œè«‹ç›´æ¥æ›è¡Œã€‚
        
        3. **çµæ§‹**ï¼š
           - Part 1: Bonus é¡Œç›®å· (å«é–‹å ´ã€3é¡Œ)ã€‚**ä¸è¦**çµ¦ç­”æ¡ˆã€‚
           - åˆ†éš”ç·š: `|||SEPARATOR|||`
           - Part 2: è§£ç­”å· (å«åƒè€ƒç­”æ¡ˆèˆ‡è§£æ)ã€‚
        """

        try:
            response = model.generate_content(prompt, safety_settings=SAFETY_SETTINGS)
            if response.text and "|||SEPARATOR|||" in response.text:
                parts = response.text.split("|||SEPARATOR|||")
                send_telegram(parts[0].strip())
                user["pending_answers"] = parts[1].strip() 
        except Exception as e:
            print(f"Error: {e}")
            send_telegram("âš ï¸ Bonus ç”Ÿæˆå¤±æ•—")

    return user

if __name__ == "__main__":
    v_data, u_data = process_data()
    u_data_updated = run_daily_quiz(v_data, u_data)
    
    # å¯«å…¥æ›´æ–°å¾Œçš„ JSON è³‡æ–™
    save_json(VOCAB_FILE, v_data)
    if u_data_updated:
        save_json(USER_DATA_FILE, u_data_updated)
        write_log_file(u_data_updated)
    else:
        save_json(USER_DATA_FILE, u_data)
        write_log_file(u_data)