import google.generativeai as genai
import requests
import os
import json
import random
import re
from datetime import datetime, timedelta

# ================= ç’°å¢ƒè®Šæ•¸ =================
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
TG_BOT_TOKEN = os.getenv("TG_BOT_TOKEN")
TG_CHAT_ID = os.getenv("TG_CHAT_ID")

VOCAB_FILE = "vocab.json"
MODEL_NAME = 'models/gemini-1.5-flash'

# å®‰å…¨è¨­å®š (é˜²æ­¢ AI æ‹’çµ•å›ç­”)
SAFETY_SETTINGS = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]

# ================= å·¥å…·å‡½å¼ =================

def load_vocab():
    default_data = {"words": []}
    if os.path.exists(VOCAB_FILE):
        try:
            with open(VOCAB_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except: return default_data
    return default_data

def save_vocab(data):
    with open(VOCAB_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def send_telegram(message):
    if not TG_BOT_TOKEN: print(f"[æ¨¡æ“¬ç™¼é€] {message[:50]}..."); return
    
    # æ¸…æ´— Markdown ç¬¦è™Ÿ
    clean_msg = message.replace("**", "").replace("##", "").replace("__", "")
    
    try:
        requests.post(f"https://api.telegram.org/bot{TG_BOT_TOKEN}/sendMessage", json={
            "chat_id": TG_CHAT_ID, "text": clean_msg
        })
    except Exception as e: print(f"TG ç™¼é€å¤±æ•—: {e}")

def normalize_text(text):
    return text.strip().replace("ã€€", " ").lower()

# ================= é‚è¼¯ï¼šè™•ç†ä½¿ç”¨è€…è¼¸å…¥ (å­˜å–®å­—) =================

def process_updates():
    print("ğŸ“¥ æª¢æŸ¥æ˜¯å¦æœ‰æ–°å–®å­—...")
    url = f"https://api.telegram.org/bot{TG_BOT_TOKEN}/getUpdates"
    
    try:
        response = requests.get(url).json()
        if "result" not in response: return load_vocab()
        
        vocab_data = load_vocab()
        is_updated = False
        updates_log = []

        for item in response["result"]:
            if str(item["message"]["chat"]["id"]) != str(TG_CHAT_ID): continue
            
            msg_time = datetime.fromtimestamp(item["message"]["date"])
            if datetime.now() - msg_time > timedelta(hours=24): continue
            
            text = item["message"].get("text", "").strip()
            
            # Regex è§£æï¼šæ¼¢å­— ç©ºç™½ å‡å ç©ºç™½ æ„æ€
            match = re.search(r"^(\S+)[ \u3000]+(\S+)[ \u3000]+(.+)$", text)
            
            if match:
                kanji, kana, meaning = match.groups()
                
                # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
                found = False
                for word in vocab_data["words"]:
                    if normalize_text(word["kanji"]) == normalize_text(kanji) and \
                       normalize_text(word["kana"]) == normalize_text(kana):
                        word["count"] = word.get("count", 0) + 1
                        word["last_review"] = str(datetime.now().date())
                        updates_log.append(f"ğŸ”„ å¼·åŒ–è¨˜æ†¶ï¼š{kanji} (ç´¯è¨ˆ {word['count']} æ¬¡)")
                        found = True
                        is_updated = True
                        break
                
                if not found:
                    new_word = {
                        "kanji": kanji,
                        "kana": kana,
                        "meaning": meaning,
                        "count": 1,
                        "added_date": str(datetime.now().date())
                    }
                    vocab_data["words"].append(new_word)
                    updates_log.append(f"âœ… æ”¶éŒ„æ–°è©ï¼š{kanji}")
                    is_updated = True

        if is_updated:
            save_vocab(vocab_data)
            if updates_log:
                send_telegram("\n".join(set(updates_log)))
        
        return vocab_data

    except Exception as e:
        print(f"Update Error: {e}")
        return load_vocab()

# ================= æ¯æ—¥ç‰¹è¨“ç”Ÿæˆ =================

def run_daily_quiz(data):
    if not data.get("words"):
        send_telegram("ğŸ“­ å–®å­—åº«æ˜¯ç©ºçš„ï¼è«‹å‚³é€å–®å­—çµ¦æˆ‘ (æ ¼å¼: æ¼¢å­— å‡å æ„æ€)")
        return

    # 1. æ¬Šé‡æŠ½æ¨£
    weights = [w.get("count", 1) * 5 for w in data["words"]]
    k = min(10, len(data["words"]))
    selected_words = random.choices(data["words"], weights=weights, k=k)
    
    word_text = "\n".join([f"{w['kanji']} ({w['kana']}) : {w['meaning']}" for w in selected_words])

    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(MODEL_NAME)

    print("ğŸ¤– AI ç”Ÿæˆæ¸¬é©—ä¸­...")
    prompt = f"""
    ä½ æ˜¯æ—¥æ–‡ N2 æ–¯å·´é”æ•™ç·´ã€‚è«‹æ ¹æ“šä»¥ä¸‹å–®å­—åº«è£½ä½œä»Šæ—¥ç‰¹è¨“ã€‚
    
    ã€ä½¿ç”¨è€…å–®å­—åº«ã€‘
    {word_text}
    
    ã€ä»»å‹™è¦æ±‚ã€‘
    1. **å…¨ç¯‡ä½¿ç”¨ç¹é«”ä¸­æ–‡**ã€‚
    2. åš´ç¦ Markdown ç²—é«” (**)ï¼Œè«‹ç”¨ Emoji æ’ç‰ˆã€‚
    3. ç¿»è­¯é¡Œè«‹ä½¿ç”¨ N2/N3 å¸¸è¦‹æ–‡æ³•ï¼Œä¸è¦å¤ªéç°¡å–®ã€‚
    
    ã€è¼¸å‡ºå…§å®¹ã€‘
    
    ğŸ§  **è‡ªå‹•è¨˜æ†¶å¼·åŒ– (é‡é»å–®å­—)**
    (å¾å–®å­—åº«æŒ‘é¸ 3 å€‹æœ€é›£çš„å­—ï¼Œæä¾›ä¾‹å¥)
    1. [æ¼¢å­—] ([å‡å]) - [æ„æ€]
       ä¾‹å¥ï¼š[æ—¥æ–‡] ([ä¸­æ–‡])
    
    âš”ï¸ **æ–¯å·´é”ç¿»è­¯ç‰¹è¨“ (å…± 10 é¡Œ)**
    
    ğŸ”¹ **Part A: æ—¥ç¿»ä¸­ (è«‹ç¿»è­¯)**
    (åˆ©ç”¨å–®å­—åº«çš„å­—ï¼Œé€  5 å€‹æ—¥æ–‡å¥å­)
    1. ...
    
    ğŸ”¹ **Part B: ä¸­ç¿»æ—¥ (è«‹è©¦è‘—ç”¨æ—¥æ–‡èªª)**
    (å‡º 5 å€‹ä¸­æ–‡å¥å­ï¼Œå¼·è¿«ä½¿ç”¨è€…å›æƒ³å–®å­—èˆ‡æ–‡æ³•)
    6. ...
    
    (æœ€å¾Œé™„ä¸Šåƒè€ƒç­”æ¡ˆèˆ‡è§£æï¼Œä½†åœ¨å‰é¢åŠ ä¸Š "--- åƒè€ƒè§£ç­” ---")
    """
    
    try:
        response = model.generate_content(prompt, safety_settings=SAFETY_SETTINGS)
        send_telegram(response.text)
    except Exception as e:
        print(f"Gemini Error: {e}")
        send_telegram("âš ï¸ AI è€å¸«ä»Šå¤©è«‹å‡äº† (API Error)")

if __name__ == "__main__":
    # 1. å…ˆè™•ç†ä½¿ç”¨è€…æ˜¨å¤©è¼¸å…¥çš„å–®å­—
    current_data = process_updates()
    
    # 2. åŸ·è¡Œæ¯æ—¥æ¸¬é©—
    run_daily_quiz(current_data)