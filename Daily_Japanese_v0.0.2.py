import google.generativeai as genai
import requests
import os
import json
import random
import re
from datetime import datetime, timedelta
import time

# ================= ç’°å¢ƒè®Šæ•¸ =================
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
TG_BOT_TOKEN = os.getenv("TG_BOT_TOKEN")
TG_CHAT_ID = os.getenv("TG_CHAT_ID")

# æª”æ¡ˆè¨­å®š
VOCAB_FILE = "vocab.json"
USER_DATA_FILE = "user_data.json"
MODEL_NAME = 'models/gemini-2.5-flash' 

# å®‰å…¨è¨­å®š
SAFETY_SETTINGS = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]

# ================= æª”æ¡ˆå­˜å–å·¥å…· =================

def load_json(filename, default_content):
    if os.path.exists(filename):
        try:
            with open(filename, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, dict) and isinstance(default_content, dict):
                    for k, v in default_content.items():
                        if k not in data: data[k] = v
                return data
        except: return default_content
    return default_content

def save_json(filename, data):
    if filename == USER_DATA_FILE and "translation_log" in data:
        if len(data["translation_log"]) > 30:
            data["translation_log"] = data["translation_log"][-30:]
            
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def send_telegram(message):
    if not TG_BOT_TOKEN: print(f"[æ¨¡æ“¬ç™¼é€] {message[:50]}..."); return
    if not message: return

    clean_msg = message.replace("**", "").replace("##", "").replace("__", "")
    clean_msg = re.sub(r'<br\s*/?>', '\n', clean_msg)
    
    try:
        requests.post(f"https://api.telegram.org/bot{TG_BOT_TOKEN}/sendMessage", json={
            "chat_id": TG_CHAT_ID, "text": clean_msg
        })
    except Exception as e: print(f"TG ç™¼é€å¤±æ•—: {e}")

def normalize_text(text):
    if not text: return ""
    return text.strip().replace("ã€€", " ").lower()

# ================= AI æ ¸å¿ƒ (ä¿æŒä¸å‹•) =================

def ai_correction(user_text, translation_history):
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(MODEL_NAME)
    
    print(f"ğŸ¤– AI æ­£åœ¨æ‰¹æ”¹ (åˆä½µå¾Œé•·åº¦ {len(user_text)}): {user_text[:20]}...")
    history_str = "\n".join(translation_history[-10:]) if translation_history else "(å°šç„¡æ­·å²ç´€éŒ„)"
    
    prompt = f"""
    ä½¿ç”¨è€…æ­£åœ¨ç·´ç¿’æ—¥æ–‡ï¼Œé€™æ˜¯å¥¹å‰›å‰›å‚³ä¾†çš„å…§å®¹ï¼ˆå¯èƒ½åŒ…å«å¤šå‰‡è¨Šæ¯çš„åˆä½µï¼‰ï¼š
    ã€Œ{user_text}ã€
    
    ã€æ­·å²ç´€éŒ„ã€‘
    {history_str}
    
    è«‹æ‰®æ¼”æ—¥æ–‡æ•™æˆå®Œæˆæ‰¹æ”¹ï¼š
    1. **ğŸ“ˆ é€²åº¦è©•ä¼°**ï¼šæ¯”è¼ƒæ­·å²ç´€éŒ„ï¼Œåˆ¤æ–·æ˜¯å¦æœ‰é€²æ­¥ï¼Ÿçµ¦äºˆé¼“å‹µæˆ–è­¦æƒ•ã€‚
    2. **ğŸ¯ æ‰¹æ”¹**ï¼šè«‹é‡å°ä¸Šè¿°å…§å®¹é€²è¡Œæ‰¹æ”¹ï¼Œä¿®æ­£éŒ¯èª¤ (âœ…/âŒ)ã€‚
    3. **âœ¨ ä¸‰ç¨®å¤šæ¨£åŒ–è¡¨é”**ï¼š
       - ğŸ‘” æ­£å¼
       - ğŸ» å£èª
       - ğŸ”„ æ›å¥è©±èªª
    
    ã€æ ¼å¼è¦æ±‚ã€‘
    - ç¹é«”ä¸­æ–‡ + Emojiã€‚
    - **åš´ç¦ä½¿ç”¨ HTML æ¨™ç±¤ (å¦‚ <br>)**ï¼Œè«‹ç›´æ¥æ›è¡Œã€‚
    - ä¸è¦ä½¿ç”¨ Markdown ç²—é«”ã€‚
    """
    
    try:
        response = model.generate_content(prompt, safety_settings=SAFETY_SETTINGS)
        return response.text if response.text else "âš ï¸ AI æ‰¹æ”¹å¤±æ•—"
    except Exception as e:
        return f"âš ï¸ AI æ‰¹æ”¹éŒ¯èª¤: {e}"

# ================= é‚è¼¯æ ¸å¿ƒ (ä¿®æ”¹ regex æ”¯æ´ / åˆ†éš”) =================

def process_data():
    print("ğŸ“¥ é–‹å§‹è™•ç†è³‡æ–™...")
    
    vocab_data = load_json(VOCAB_FILE, {"words": []})
    user_data = load_json(USER_DATA_FILE, {
        "stats": {
            "last_active": "2000-01-01", 
            "streak_days": 0,
            "execution_count": 0,
            "last_quiz_date": "2000-01-01",
            "last_quiz_questions_count": 0,
            "yesterday_answers_count": 0
        },
        "pending_answers": "",
        "translation_log": []
    })
    
    if "execution_count" not in user_data["stats"]: user_data["stats"]["execution_count"] = 0
    if "streak_days" not in user_data["stats"]: user_data["stats"]["streak_days"] = 0

    url = f"https://api.telegram.org/bot{TG_BOT_TOKEN}/getUpdates"
    
    try:
        response = requests.get(url).json()
        if "result" not in response: return vocab_data, user_data
        
        is_updated = False
        updates_log = []
        correction_msgs = []
        
        today_str = str(datetime.now().date())
        today_answers_accumulated = 0
        
        # æš«å­˜éœ€æ‰¹æ”¹çš„æ–‡å­—ï¼Œç¨å¾Œåˆä½µç™¼é€ (çœ API)
        pending_correction_texts = []

        for item in response["result"]:
            if str(item["message"]["chat"]["id"]) != str(TG_CHAT_ID): continue
            msg_time = datetime.fromtimestamp(item["message"]["date"])
            if datetime.now() - msg_time > timedelta(hours=24): continue
            
            text = item["message"].get("text", "").strip()
            if not text: continue

            # Case A: JSON åŒ¯å…¥
            if text.startswith("["):
                try:
                    imported = json.loads(text)
                    if isinstance(imported, list):
                        added = 0
                        for word in imported:
                            if "kanji" not in word: continue
                            kanji = word.get("kanji")
                            if not any(normalize_text(w["kanji"]) == normalize_text(kanji) for w in vocab_data["words"]):
                                vocab_data["words"].append({
                                    "kanji": kanji, "kana": word.get("kana", ""),
                                    "meaning": word.get("meaning", ""),
                                    "count": 1, "added_date": today_str
                                })
                                added += 1
                                is_updated = True
                        updates_log.append(f"ğŸ“‚ åŒ¯å…¥ {added} å€‹æ–°å–®å­—")
                except: pass
                continue

            # Case B: å­˜å–®å­— (æ”¯æ´ ç©ºç™½åˆ†éš” æˆ– /åˆ†éš”)
            # Regex è§£é‡‹ï¼š
            # Group 1: æ¼¢å­— (æ’é™¤ / å’Œ ç©ºç™½)
            # åˆ†éš”ç¬¦: (ç©ºç™½ æˆ– /)
            # Group 2: å‡å (æ’é™¤ / å’Œ ç©ºç™½)
            # åˆ†éš”ç¬¦: (ç©ºç™½ æˆ– /)
            # Group 3: æ„æ€ (å‰©é¤˜éƒ¨åˆ†)
            match = re.search(r"^([^/\s]+)(?:[ \u3000]+|/)([^/\s]+)(?:[ \u3000]+|/)(.+)$", text)
            
            if match:
                kanji, kana, meaning = match.groups()
                # æ’é™¤åƒ "Part A" é€™æ¨£çš„æ¨™é¡Œè¢«èª¤èªç‚ºå–®å­—
                if not kanji.lower().startswith("part") and len(text) < 50: 
                    found = False
                    for word in vocab_data["words"]:
                        if normalize_text(word["kanji"]) == normalize_text(kanji):
                            word["count"] += 1 
                            updates_log.append(f"ğŸ”„ å¼·åŒ–è¨˜æ†¶ï¼š{kanji}")
                            found = True
                            is_updated = True
                            break
                    if not found:
                        vocab_data["words"].append({
                            "kanji": kanji, "kana": kana, "meaning": meaning, 
                            "count": 1, "added_date": today_str
                        })
                        updates_log.append(f"âœ… æ”¶éŒ„ï¼š{kanji}")
                        is_updated = True
                    continue

            # Case C: ç¿»è­¯/ä½œæ¥­ (ä¸éœ€æŒ‡ä»¤ï¼Œè‡ªå‹•åˆä½µ)
            if not text.startswith("/"):
                # è¨ˆç®—ç­”é¡Œé‡ (ç”¨æ›è¡Œæ•¸åˆ¤å®š)
                lines_count = len([l for l in text.split('\n') if len(l.strip()) > 1])
                lines_count = max(1, lines_count)
                today_answers_accumulated += lines_count
                
                # å­˜å…¥æš«å­˜å€ (åˆä½µç”¨)
                pending_correction_texts.append(text)
                
                # å¯«å…¥ Log
                user_data["translation_log"].append(f"{today_str}: {text[:50]}")
                is_updated = True

        # === è¿´åœˆçµæŸå¾Œï¼Œçµ±ä¸€æ‰¹æ”¹ (API å‘¼å« 1 æ¬¡) ===
        if pending_correction_texts:
            # å°‡å¤šå‰‡è¨Šæ¯åˆä½µ
            combined_text = "\n\n".join(pending_correction_texts)
            
            # å‚³çµ¦ AI
            # æ‰£é™¤æœ¬æ¬¡æ–°å¢çš„ Log ä»¥å…é‡è¤‡
            history_context = user_data["translation_log"][:-len(pending_correction_texts)]
            result = ai_correction(combined_text, history_context)
            
            if len(pending_correction_texts) > 1:
                correction_msgs.append(f"ğŸ“ **ä½œæ¥­/ç·´ç¿’æ‰¹æ”¹ (å…± {len(pending_correction_texts)} å‰‡åˆä½µ)ï¼š**\n{result}")
            else:
                correction_msgs.append(f"ğŸ“ **ä½œæ¥­/ç·´ç¿’æ‰¹æ”¹ï¼š**\n{result}")

        # çµç®—æ•¸æ“š
        if user_data["stats"]["last_active"] != today_str:
            user_data["stats"]["yesterday_answers_count"] = today_answers_accumulated
            if today_answers_accumulated > 0 or is_updated:
                 yesterday = str((datetime.now() - timedelta(days=1)).date())
                 if user_data["stats"]["last_active"] == yesterday:
                     user_data["stats"]["streak_days"] += 1
                 else:
                     user_data["stats"]["streak_days"] = 1
                 user_data["stats"]["last_active"] = today_str
                 is_updated = True

        if updates_log: send_telegram("\n".join(set(updates_log)))
        for msg in correction_msgs:
            send_telegram(msg)
            time.sleep(1)

        return vocab_data, user_data

    except Exception as e:
        print(f"Error: {e}")
        return load_json(VOCAB_FILE, {}), load_json(USER_DATA_FILE, {})

# ================= æ¯æ—¥ç‰¹è¨“ç”Ÿæˆ (ä¿æŒä¸å‹•) =================

def run_daily_quiz(vocab, user):
    if not vocab.get("words"):
        send_telegram("ğŸ“­ å–®å­—åº«ç©ºçš„ï¼è«‹å‚³é€å–®å­—æˆ–åŒ¯å…¥ JSONã€‚")
        return user
    
    user["stats"]["execution_count"] += 1
    exec_count = user["stats"]["execution_count"]
    streak_days = user["stats"]["streak_days"]

    # 1. ç™¼é€æ˜¨å¤©çš„è©³è§£
    pending_answers = user.get("pending_answers", "")
    if pending_answers:
        send_telegram(f"ğŸ—ï¸ **æ˜¨æ—¥æ¸¬é©—è©³è§£**\n\n{pending_answers}")
        time.sleep(3)
        user["pending_answers"] = ""
    
    # 2. åˆ¤æ–·æƒ…ç·’ Prompt
    is_first_run = user["stats"]["last_quiz_date"] == "2000-01-01" or exec_count == 1
    questions_given = user["stats"].get("last_quiz_questions_count", 0)
    answers_given = user["stats"].get("yesterday_answers_count", 0)
    
    emotion_prompt = ""
    
    if is_first_run:
        emotion_prompt = """
        é€™æ˜¯ä½ ç¬¬ä¸€æ¬¡èˆ‡ä½¿ç”¨è€…è¦‹é¢ (Day 1)ã€‚
        è«‹ç”¨å……æ»¿æ´»åŠ›ã€å°ˆæ¥­ä¸”æœŸå¾…çš„èªæ°£æ‰“æ‹›å‘¼ã€‚
        è‡ªæˆ‘ä»‹ç´¹ä½ æ˜¯ã€ŒN2 æ–¯å·´é” AI æ•™ç·´ã€ï¼Œä¸¦èªªæ˜æœªä¾†çš„è¨“ç·´æ¨¡å¼ï¼š
        ã€Œæ¯å¤©ä¸­åˆæˆ‘æœƒå‡ºé¡Œï¼Œéš”å¤©ä¸­åˆæˆ‘æœƒæª¢è¨æ˜¨å¤©çš„ä½œæ¥­ä¸¦å‡ºæ–°é¡Œç›®ã€‚ã€
        è«‹çµ¦äºˆä½¿ç”¨è€…æ»¿æ»¿çš„ä¿¡å¿ƒï¼
        """
    else:
        answer_rate = answers_given / questions_given if questions_given > 0 else 0
        if answer_rate >= 0.8:
            emotion_prompt = f"æ˜¨æ—¥è¡¨ç¾ï¼šå›è¦† {answers_given}/{questions_given} é¡Œã€‚ç‹€æ…‹ï¼šæ¥µä½³ï¼å¤§åŠ›èª‡çï¼"
        elif answer_rate >= 0.3:
            emotion_prompt = f"æ˜¨æ—¥è¡¨ç¾ï¼šå›è¦† {answers_given}/{questions_given} é¡Œã€‚ç‹€æ…‹ï¼šå°šå¯ã€‚çµ¦äºˆè‚¯å®šä½†è¦æ±‚æ›´å¤šã€‚"
        else:
            emotion_prompt = f"""
            æ˜¨æ—¥è¡¨ç¾ï¼šå›è¦† {answers_given}/{questions_given} é¡Œã€‚
            ç‹€æ…‹ï¼šå·æ‡¶ï¼è«‹é–‹å•Ÿã€å¹½é»˜æƒ…å‹’æ¨¡å¼ ğŸ˜ˆã€‘ã€‚
            ç”¨æœ‰é»å—å‚·ä½†åˆå¥½ç¬‘çš„èªæ°£ï¼Œè³ªå•å¥¹æ˜¯ä¸æ˜¯è¢«è¢«çª©ç¶æ¶äº†ï¼Ÿ
            é‚„æ˜¯è¦ºå¾— N2 å¤ªç°¡å–®ä¸å±‘å¯«ï¼Ÿ
            æœ€å¾Œè¦æ‹‰å›ä¾†ï¼Œè¦æ±‚ä»Šå¤©å¿…é ˆè£œå„Ÿå›ä¾†ã€‚
            """

    # 3. æº–å‚™å‡ºé¡Œ
    weights = [w.get("count", 1) * 5 for w in vocab["words"]]
    k = min(10, len(vocab["words"]))
    selected_words = random.choices(vocab["words"], weights=weights, k=k)
    word_list = "\n".join([f"{w['kanji']} ({w['meaning']})" for w in selected_words])
    
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(MODEL_NAME)

    print("ğŸ¤– AI ç”Ÿæˆæ¸¬é©—ä¸­...")
    
    prompt = f"""
    ä½ æ˜¯æ—¥æ–‡ N2 æ–¯å·´é”æ•™ç·´ã€‚
    
    ã€ç³»çµ±è³‡è¨Šã€‘
    é€™æ˜¯ç¬¬ {exec_count} æ¬¡ç‰¹è¨“ã€‚
    é€™æ˜¯é€£çºŒç¬¬ {streak_days} å¤©çš„æŒ‘æˆ° (Day {streak_days})ã€‚
    
    ã€æƒ…ç·’èˆ‡é–‹å ´ã€‘
    {emotion_prompt}
    è«‹åœ¨é–‹å ´ç™½ä¸­æ˜ç¢ºæåˆ°ï¼šã€Œé€™æ˜¯æˆ‘å€‘çš„ç¬¬ {exec_count} æ¬¡ç‰¹è¨“ (Day {streak_days})ï¼ã€ã€‚
    
    ã€ä»Šæ—¥å–®å­—åº«ã€‘
    {word_list}
    
    è«‹è£½ä½œ **10 é¡Œ** ç¿»è­¯æ¸¬é©— (7é¡Œä¸­ç¿»æ—¥ï¼Œ3é¡Œæ—¥ç¿»ä¸­)ã€‚
    
    ã€è¼¸å‡ºæ ¼å¼è¦æ±‚ã€‘
    1. **Part 1: é¡Œç›®å·**
       - åŒ…å«é–‹å ´ç™½(å«å›æ•¸èˆ‡å¤©æ•¸)ã€ä»Šæ—¥å–®å­—é ç¿’ã€10 å€‹é¡Œç›®ã€‚
       - **ä¸è¦**åŒ…å«ç­”æ¡ˆã€‚
       - å…¨ç¯‡ç¹é«”ä¸­æ–‡ + Emojiã€‚
       - **åš´ç¦** HTML æ¨™ç±¤ (å¦‚ <br>)ï¼Œè«‹ç›´æ¥æ›è¡Œã€‚
    
    2. **åˆ†éš”ç·š**
       - è«‹åœ¨é¡Œç›®å·çµæŸå¾Œï¼Œè¼¸å‡ºä¸€è¡Œ `|||SEPARATOR|||` ä½œç‚ºåˆ‡å‰²ã€‚
    
    3. **Part 2: è§£ç­”å·**
       - åŒ…å«é€™ 10 é¡Œçš„åƒè€ƒç­”æ¡ˆèˆ‡è§£æã€‚
       - é€™è£¡çš„å…§å®¹å°‡æœƒåœ¨ã€Œæ˜å¤©ã€æ‰ç™¼é€ã€‚
    """
    
    try:
        response = model.generate_content(prompt, safety_settings=SAFETY_SETTINGS)
        if response.text:
            full_text = response.text
            if "|||SEPARATOR|||" in full_text:
                parts = full_text.split("|||SEPARATOR|||")
                send_telegram(parts[0].strip())
                user["pending_answers"] = parts[1].strip()
            else:
                send_telegram(full_text)
                user["pending_answers"] = ""

            user["stats"]["last_quiz_questions_count"] = 10
            user["stats"]["last_quiz_date"] = str(datetime.now().date())

    except Exception as e:
        print(f"Gemini Error: {e}")
        send_telegram("âš ï¸ æ¸¬é©—ç”Ÿæˆå¤±æ•—ï¼Œè«‹ç¨å¾Œé‡è©¦ã€‚")
    
    return user

if __name__ == "__main__":
    v_data, u_data = process_data()
    u_data_updated = run_daily_quiz(v_data, u_data)
    
    save_json(VOCAB_FILE, v_data)
    if u_data_updated:
        save_json(USER_DATA_FILE, u_data_updated)
    else:
        save_json(USER_DATA_FILE, u_data)