import google.generativeai as genai
import requests
import os
import json
import random
import re
from datetime import datetime, timedelta
import time

# ================= ç’°å¢ƒè®Šæ•¸ =================
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
TG_BOT_TOKEN = os.getenv("TG_BOT_TOKEN")
TG_CHAT_ID = os.getenv("TG_CHAT_ID")

# æª”æ¡ˆè¨­å®š
VOCAB_FILE = "vocab.json"
USER_DATA_FILE = "user_data.json"
MODEL_NAME = 'models/gemini-2.5-flash' 

# å®‰å…¨è¨­å®š
SAFETY_SETTINGS = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]

# ================= æª”æ¡ˆå­˜å–å·¥å…· =================

def load_json(filename, default_content):
    if os.path.exists(filename):
        try:
            with open(filename, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, dict) and isinstance(default_content, dict):
                    for k, v in default_content.items():
                        if k not in data: data[k] = v
                return data
        except: return default_content
    return default_content

def save_json(filename, data):
    if filename == USER_DATA_FILE and "translation_log" in data:
        if len(data["translation_log"]) > 30:
            data["translation_log"] = data["translation_log"][-30:]
            
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def send_telegram(message):
    if not TG_BOT_TOKEN: print(f"[æ¨¡æ“¬ç™¼é€] {message[:50]}..."); return
    if not message: return

    clean_msg = message.replace("**", "").replace("##", "").replace("__", "")
    clean_msg = re.sub(r'<br\s*/?>', '\n', clean_msg)
    
    try:
        requests.post(f"https://api.telegram.org/bot{TG_BOT_TOKEN}/sendMessage", json={
            "chat_id": TG_CHAT_ID, "text": clean_msg
        })
    except Exception as e: print(f"TG ç™¼é€å¤±æ•—: {e}")

def normalize_text(text):
    if not text: return ""
    return text.strip().replace("ã€€", " ").lower()

# ================= AI æ ¸å¿ƒ =================

def ai_correction(user_text, translation_history):
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(MODEL_NAME)
    
    print(f"ğŸ¤– AI æ­£åœ¨æ‰¹æ”¹ (åˆä½µå¾Œé•·åº¦ {len(user_text)}): {user_text[:20]}...")
    history_str = "\n".join(translation_history[-10:]) if translation_history else "(å°šç„¡æ­·å²ç´€éŒ„)"
    
    # ğŸ“ æ‰¹æ”¹æç¤ºè©ï¼šç¶­æŒè©³ç´°ç‰ˆ
    prompt = f"""
    ä½¿ç”¨è€…æ­£åœ¨ç·´ç¿’æ—¥æ–‡ï¼Œé€™æ˜¯å¥¹å‰›å‰›å‚³ä¾†çš„å…§å®¹ï¼ˆå¯èƒ½åŒ…å«å¤šå‰‡è¨Šæ¯çš„åˆä½µï¼‰ï¼š
    ã€Œ{user_text}ã€
    
    ã€æ­·å²ç´€éŒ„ã€‘
    {history_str}
    
    è«‹æ‰®æ¼”æ—¥æ–‡æ•™æˆå®Œæˆæ‰¹æ”¹ï¼š
    1. **ğŸ“ˆ é€²åº¦è©•ä¼°**ï¼šæ¯”è¼ƒæ­·å²ç´€éŒ„ï¼Œåˆ¤æ–·æ˜¯å¦æœ‰é€²æ­¥ï¼Ÿçµ¦äºˆé¼“å‹µæˆ–è­¦æƒ•ã€‚
    2. **ğŸ¯ æ‰¹æ”¹**ï¼šè«‹é‡å°ä¸Šè¿°å…§å®¹é€²è¡Œæ‰¹æ”¹ï¼Œä¿®æ­£éŒ¯èª¤ (âœ…/âŒ)ã€‚
    3. **âœ¨ ä¸‰ç¨®å¤šæ¨£åŒ–è¡¨é”**ï¼š
       - ğŸ‘” æ­£å¼
       - ğŸ» å£èª
       - ğŸ”„ æ›å¥è©±èªª
    
    ã€æ ¼å¼åš´æ ¼è¦æ±‚ã€‘
    1. **èªè¨€**ï¼šè§£èªªèˆ‡è©•èªè«‹å…¨ç¨‹ä½¿ç”¨ã€Œç¹é«”ä¸­æ–‡ã€(Traditional Chinese)ã€‚
    2. **æ’ç‰ˆ**ï¼š
       - **åš´ç¦** ä½¿ç”¨ Markdown æ¨™é¡Œ (å¦‚ # æˆ– ##)ã€‚
       - è«‹ä½¿ç”¨ Emoji (å¦‚ ğŸ“ˆ, ğŸ¯, âœ¨, ğŸ‘”, ğŸ», ğŸ”„, âœ…, âŒ) ä¾†å€éš”æ®µè½èˆ‡é …ç›®ã€‚
       - **åš´ç¦** ä½¿ç”¨ HTML æ¨™ç±¤ (å¦‚ <br>)ï¼Œè«‹ç›´æ¥æ›è¡Œã€‚
    """
    
    try:
        response = model.generate_content(prompt, safety_settings=SAFETY_SETTINGS)
        return response.text if response.text else "âš ï¸ AI æ‰¹æ”¹å¤±æ•—"
    except Exception as e:
        return f"âš ï¸ AI æ‰¹æ”¹éŒ¯èª¤: {e}"

# ================= é‚è¼¯æ ¸å¿ƒ =================

def process_data():
    print("ğŸ“¥ é–‹å§‹è™•ç†è³‡æ–™...")
    
    vocab_data = load_json(VOCAB_FILE, {"words": []})
    user_data = load_json(USER_DATA_FILE, {
        "stats": {
            "last_active": "2000-01-01", 
            "streak_days": 0,
            "execution_count": 0,
            "last_quiz_date": "2000-01-01",
            "last_quiz_questions_count": 0,
            "daily_answers_count": 0,
            "bonus_answers_count": 0,
            "yesterday_main_score": 0,
            "yesterday_bonus_score": 0,
            "last_update_id": 0
        },
        "pending_answers": "",
        "translation_log": []
    })
    
    stats = user_data["stats"]
    for key in ["daily_answers_count", "bonus_answers_count", "yesterday_main_score", 
                "yesterday_bonus_score", "execution_count", "streak_days", "last_update_id"]:
        if key not in stats: stats[key] = 0

    url = f"https://api.telegram.org/bot{TG_BOT_TOKEN}/getUpdates"
    
    try:
        response = requests.get(url).json()
        if "result" not in response: return vocab_data, user_data
        
        is_updated = False
        updates_log = []
        correction_msgs = []
        
        today_str = str(datetime.now().date())
        today_answers_detected = 0
        
        pending_correction_texts = []
        
        last_processed_id = user_data["stats"]["last_update_id"]
        is_fresh_start = (last_processed_id == 0)
        max_id_in_this_run = last_processed_id

        for item in response["result"]:
            current_update_id = item["update_id"]
            if current_update_id <= last_processed_id: continue
            if current_update_id > max_id_in_this_run: max_id_in_this_run = current_update_id

            if str(item["message"]["chat"]["id"]) != str(TG_CHAT_ID): continue
            
            msg_time = datetime.fromtimestamp(item["message"]["date"])
            if datetime.now() - msg_time > timedelta(hours=24): continue
            
            text = item["message"].get("text", "").strip()
            if not text: continue

            # Case A: JSON åŒ¯å…¥
            if text.startswith("["):
                try:
                    imported = json.loads(text)
                    if isinstance(imported, list):
                        added = 0
                        for word in imported:
                            if "kanji" not in word: continue
                            kanji = word.get("kanji")
                            if not any(normalize_text(w["kanji"]) == normalize_text(kanji) for w in vocab_data["words"]):
                                vocab_data["words"].append({
                                    "kanji": kanji, "kana": word.get("kana", ""),
                                    "meaning": word.get("meaning", ""),
                                    "count": 1, "added_date": today_str
                                })
                                added += 1
                                is_updated = True
                        updates_log.append(f"ğŸ“‚ åŒ¯å…¥ {added} å€‹æ–°å–®å­—")
                except: pass
                continue

            # Case B: å­˜å–®å­—
            match = re.search(r"^([^/\s]+)(?:[ \u3000]+|/)([^/\s]+)(?:[ \u3000]+|/)(.+)$", text)
            if match:
                if is_fresh_start: continue
                kanji, kana, meaning = match.groups()
                if not kanji.lower().startswith("part") and len(text) < 50: 
                    found = False
                    for word in vocab_data["words"]:
                        if normalize_text(word["kanji"]) == normalize_text(kanji):
                            word["count"] += 1 
                            updates_log.append(f"ğŸ”„ å¼·åŒ–è¨˜æ†¶ï¼š{kanji}")
                            found = True
                            is_updated = True
                            break
                    if not found:
                        vocab_data["words"].append({
                            "kanji": kanji, "kana": kana, "meaning": meaning, 
                            "count": 1, "added_date": today_str
                        })
                        updates_log.append(f"âœ… æ”¶éŒ„ï¼š{kanji}")
                        is_updated = True
                    continue

            # Case C: ç¿»è­¯/ä½œæ¥­
            if not text.startswith("/"):
                if is_fresh_start: continue
                lines_count = len([l for l in text.split('\n') if len(l.strip()) > 1])
                lines_count = max(1, lines_count)
                today_answers_detected += lines_count
                
                pending_correction_texts.append(text)
                user_data["translation_log"].append(f"{today_str}: {text[:50]}")
                is_updated = True

        # === æ‰¹æ”¹è™•ç† ===
        if is_fresh_start:
            if max_id_in_this_run > user_data["stats"]["last_update_id"]:
                user_data["stats"]["last_update_id"] = max_id_in_this_run
                is_updated = True
            print("ğŸš€ åˆå§‹åŒ–å®Œæˆï¼šå¿½ç•¥èˆŠæœ‰è¨Šæ¯ã€‚")
        else:
            if pending_correction_texts:
                combined_text = "\n\n".join(pending_correction_texts)
                history_context = user_data["translation_log"][:-len(pending_correction_texts)]
                result = ai_correction(combined_text, history_context)
                
                title_text = f"ğŸ“ **ä½œæ¥­/ç·´ç¿’æ‰¹æ”¹ (å…± {len(pending_correction_texts)} å‰‡åˆä½µ)ï¼š**" if len(pending_correction_texts) > 1 else "ğŸ“ **ä½œæ¥­/ç·´ç¿’æ‰¹æ”¹ï¼š**"
                correction_msgs.append(f"{title_text}\n{result}")

            if max_id_in_this_run > user_data["stats"]["last_update_id"]:
                user_data["stats"]["last_update_id"] = max_id_in_this_run
                is_updated = True

        # === è¨ˆåˆ†é‚è¼¯ ===
        if today_answers_detected > 0:
            current_main = user_data["stats"]["daily_answers_count"]
            main_quota = 10 
            remaining_quota = max(0, main_quota - current_main)
            
            fill_main = min(today_answers_detected, remaining_quota)
            user_data["stats"]["daily_answers_count"] += fill_main
            
            spill_to_bonus = today_answers_detected - fill_main
            if spill_to_bonus > 0:
                user_data["stats"]["bonus_answers_count"] += spill_to_bonus
            is_updated = True

        # Streak æ›´æ–°
        if user_data["stats"]["last_active"] != today_str:
            if today_answers_detected > 0 or is_updated:
                 yesterday = str((datetime.now() - timedelta(days=1)).date())
                 if user_data["stats"]["last_active"] == yesterday:
                     user_data["stats"]["streak_days"] += 1
                 else:
                     user_data["stats"]["streak_days"] = 1
                 user_data["stats"]["last_active"] = today_str
                 is_updated = True

        if updates_log: send_telegram("\n".join(set(updates_log)))
        for msg in correction_msgs:
            send_telegram(msg)
            time.sleep(1)

        return vocab_data, user_data

    except Exception as e:
        print(f"Error: {e}")
        return load_json(VOCAB_FILE, {}), load_json(USER_DATA_FILE, {})

# ================= æ¯æ—¥ç‰¹è¨“ç”Ÿæˆ (Prompt ä¿®æ­£ç‰ˆ) =================

def run_daily_quiz(vocab, user):
    if not vocab.get("words"):
        send_telegram("ğŸ“­ å–®å­—åº«ç©ºçš„ï¼è«‹å‚³é€å–®å­—æˆ–åŒ¯å…¥ JSONã€‚")
        return user
    
    # 1. æ¯æ¬¡åŸ·è¡Œå…ˆç™¼è©³è§£
    pending_answers = user.get("pending_answers", "")
    if pending_answers:
        send_telegram(f"ğŸ—ï¸ **å‰æ¬¡æ¸¬é©—è©³è§£**\n\n{pending_answers}")
        time.sleep(3)
        user["pending_answers"] = ""
    
    today_str = str(datetime.now().date())
    is_new_day = (user["stats"]["last_quiz_date"] != today_str)

    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(MODEL_NAME)
    
    weights = [w.get("count", 1) * 5 for w in vocab["words"]]
    k = min(10, len(vocab["words"]))
    selected_words = random.choices(vocab["words"], weights=weights, k=k)
    word_list = "\n".join([f"{w['kanji']} ({w['meaning']})" for w in selected_words])

    # ================= Scenario A: æ–°çš„ä¸€å¤© (æ¯æ—¥å¿…ä¿®) =================
    if is_new_day:
        user["stats"]["yesterday_main_score"] = user["stats"]["daily_answers_count"]
        user["stats"]["yesterday_bonus_score"] = user["stats"]["bonus_answers_count"]
        user["stats"]["daily_answers_count"] = 0
        user["stats"]["bonus_answers_count"] = 0
        
        user["stats"]["execution_count"] += 1
        exec_count = user["stats"]["execution_count"]
        streak_days = user["stats"]["streak_days"]
        
        is_first_run = (user["stats"]["last_quiz_date"] == "2000-01-01") or (exec_count == 1)
        main_score = user["stats"]["yesterday_main_score"]
        bonus_score = user["stats"]["yesterday_bonus_score"]
        
        emotion_prompt = ""
        if is_first_run:
            emotion_prompt = """
            é€™æ˜¯ä½ ç¬¬ä¸€æ¬¡èˆ‡ä½¿ç”¨è€…è¦‹é¢ (Day 1)ã€‚
            **è«‹æ³¨æ„ï¼šè«‹å…¨ç¨‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ (Traditional Chinese)ã€‚**
            è«‹ç”¨å……æ»¿æ´»åŠ›ã€å°ˆæ¥­ä¸”æœŸå¾…çš„èªæ°£æ‰“æ‹›å‘¼ã€‚
            è‡ªæˆ‘ä»‹ç´¹ä½ æ˜¯ã€ŒN2 æ–¯å·´é” AI æ•™ç·´ã€ï¼Œä¸¦èªªæ˜æœªä¾†çš„è¨“ç·´æ¨¡å¼ï¼š
            ã€Œæ¯å¤©ä¸­åˆæˆ‘æœƒå‡ºé¡Œï¼Œéš”å¤©ä¸­åˆæˆ‘æœƒæª¢è¨æ˜¨å¤©çš„ä½œæ¥­ä¸¦å‡ºæ–°é¡Œç›®ã€‚ã€
            è«‹çµ¦äºˆä½¿ç”¨è€…æ»¿æ»¿çš„ä¿¡å¿ƒï¼
            """
        else:
            answer_rate = main_score / 10
            if answer_rate >= 0.8:
                if bonus_score > 0:
                    emotion_prompt = f"æ˜¨æ—¥è¡¨ç¾ï¼šå¿…ä¿® {main_score}/10ï¼ŒBonus {bonus_score}ã€‚ç‹€æ…‹ï¼šç¥ä¸€èˆ¬çš„è‡ªå¾‹ï¼è«‹ç”¨æ¥µåº¦å´‡æ‹œèªæ°£èª‡çï¼"
                else:
                    emotion_prompt = f"æ˜¨æ—¥è¡¨ç¾ï¼šå¿…ä¿® {main_score}/10ã€‚ç‹€æ…‹ï¼šå„ªç§€ã€‚çµ¦äºˆé«˜åº¦è‚¯å®šã€‚"
            elif answer_rate >= 0.3:
                emotion_prompt = f"æ˜¨æ—¥è¡¨ç¾ï¼šå¿…ä¿® {main_score}/10ã€‚ç‹€æ…‹ï¼šå°šå¯ã€‚æé†’è¦æ›´åŠªåŠ›ã€‚"
            else:
                emotion_prompt = f"""
                æ˜¨æ—¥è¡¨ç¾ï¼šå¿…ä¿® {main_score}/10ã€‚ç‹€æ…‹ï¼šå·æ‡¶ï¼
                è«‹é–‹å•Ÿã€å¹½é»˜æƒ…å‹’æ¨¡å¼ ğŸ˜ˆã€‘ã€‚
                ç”¨æœ‰é»å—å‚·ä½†åˆå¥½ç¬‘çš„èªæ°£ï¼Œè³ªå•å¥¹æ˜¯ä¸æ˜¯è¢«è¢«çª©ç¶æ¶äº†ï¼Ÿ
                é‚„æ˜¯è¦ºå¾— N2 å¤ªç°¡å–®ä¸å±‘å¯«ï¼Ÿ
                **è«‹å…¨ç¨‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚**
                """

        print("ğŸ¤– ç”Ÿæˆæ¯æ—¥å¿…ä¿® (10é¡Œ)...")
        
        # ğŸ”¥ğŸ”¥ğŸ”¥ ä¿®æ­£é»ï¼šä½¿ç”¨ v0.0.5 çš„è©³ç´°æç¤ºè© ğŸ”¥ğŸ”¥ğŸ”¥
        prompt = f"""
        ä½ æ˜¯æ—¥æ–‡ N2 æ–¯å·´é”æ•™ç·´ã€‚
        
        ã€ç³»çµ±è³‡è¨Šã€‘
        é€™æ˜¯ç¬¬ {exec_count} æ¬¡ç‰¹è¨“ã€‚
        é€™æ˜¯é€£çºŒç¬¬ {streak_days} å¤©çš„æŒ‘æˆ° (Day {streak_days})ã€‚
        
        ã€æƒ…ç·’èˆ‡é–‹å ´ã€‘
        {emotion_prompt}
        è«‹åœ¨é–‹å ´ç™½ä¸­æ˜ç¢ºæåˆ°ï¼šã€Œé€™æ˜¯æˆ‘å€‘çš„ç¬¬ {exec_count} æ¬¡ç‰¹è¨“ (Day {streak_days})ï¼ã€ã€‚
        
        ã€ä»Šæ—¥å–®å­—åº«ã€‘
        {word_list}
        
        è«‹è£½ä½œ **10 é¡Œ** ç¿»è­¯æ¸¬é©— (7é¡Œä¸­ç¿»æ—¥ï¼Œ3é¡Œæ—¥ç¿»ä¸­)ã€‚
        
        ã€è¼¸å‡ºæ ¼å¼è¦æ±‚ (åš´æ ¼éµå®ˆ)ã€‘
        1. **èªè¨€**ï¼š
           - é–‹å ´ç™½ã€å–®å­—é ç¿’ã€é¡Œç›®èªªæ˜ï¼š**å…¨ç¨‹ä½¿ç”¨ç¹é«”ä¸­æ–‡**ã€‚
           - é¡Œç›®æœ¬èº«ï¼šæ—¥æ–‡æˆ–ä¸­æ–‡ã€‚
        
        2. **æ’ç‰ˆ**ï¼š
           - **åš´ç¦** ä½¿ç”¨ Markdown æ¨™é¡Œ (å¦‚ # æˆ– ##)ã€‚
           - è«‹ä½¿ç”¨ Emoji (å¦‚ âš”ï¸, ğŸ“š, ğŸ“, ğŸ”¹) ä¾†å€éš”æ®µè½èˆ‡é …ç›®ã€‚
           - **åš´ç¦** ä½¿ç”¨ HTML æ¨™ç±¤ (å¦‚ <br>)ï¼Œè«‹ç›´æ¥æ›è¡Œã€‚
        
        3. **çµæ§‹**ï¼š
           - Part 1: é¡Œç›®å· (å«é–‹å ´ã€å–®å­—ã€10é¡Œ)ã€‚**ä¸è¦**çµ¦ç­”æ¡ˆã€‚
           - åˆ†éš”ç·š: `|||SEPARATOR|||`
           - Part 2: è§£ç­”å· (å«åƒè€ƒç­”æ¡ˆèˆ‡è§£æ)ã€‚
        """
        
        try:
            response = model.generate_content(prompt, safety_settings=SAFETY_SETTINGS)
            if response.text and "|||SEPARATOR|||" in response.text:
                parts = response.text.split("|||SEPARATOR|||")
                send_telegram(parts[0].strip())
                user["pending_answers"] = parts[1].strip()
                
                user["stats"]["last_quiz_date"] = today_str
                user["stats"]["last_quiz_questions_count"] = 10
        except Exception as e:
            print(f"Error: {e}")
            send_telegram("âš ï¸ æ¸¬é©—ç”Ÿæˆå¤±æ•—")

    # ================= Scenario B: Bonus æ¨¡å¼ =================
    else:
        print("ğŸ¤– ç”Ÿæˆ Bonus æŒ‘æˆ° (3é¡Œ)...")
        
        # ğŸ“ Bonus æç¤ºè© (å·²ç¶“æ˜¯è©³ç´°ç‰ˆï¼Œç¶­æŒä¸è®Š)
        prompt = f"""
        ä½ æ˜¯æ—¥æ–‡ N2 æ–¯å·´é”æ•™ç·´ã€‚
        ä½¿ç”¨è€…ä»Šå¤©å·²ç¶“é ˜éæ¯æ—¥ä½œæ¥­äº†ï¼Œä½†å¥¹**ä¸»å‹•**å†æ¬¡åŸ·è¡Œç¨‹å¼ï¼Œè¡¨ç¤ºå¥¹æƒ³è¦æ›´å¤šç·´ç¿’ï¼
        
        è«‹ç”¨ã€Œé©šå–œã€è®šå˜†ã€çš„èªæ°£ï¼Œç¨±è®šå¥¹çš„ç©æ¥µåº¦ï¼ˆä¾‹å¦‚ï¼šã€Œå¤©å•Šï¼å¯«å®Œä½œæ¥­é‚„ä¸å¤ ï¼Ÿç«Ÿç„¶é‚„è¦åŠ ç·´ï¼Ÿã€ï¼‰ã€‚
        ä¸¦æä¾› **3 é¡Œ** é«˜é›£åº¦çš„ N2 ç¿»è­¯æŒ‘æˆ° (Bonus Challenge)ã€‚
        
        ã€ä»Šæ—¥å–®å­—åº«ã€‘
        {word_list}
        
        ã€è¼¸å‡ºæ ¼å¼è¦æ±‚ (åš´æ ¼éµå®ˆ)ã€‘
        1. **èªè¨€**ï¼š
           - é–‹å ´ç™½ã€é¡Œç›®èªªæ˜ï¼š**å…¨ç¨‹ä½¿ç”¨ç¹é«”ä¸­æ–‡**ã€‚
        
        2. **æ’ç‰ˆ**ï¼š
           - **åš´ç¦** ä½¿ç”¨ Markdown æ¨™é¡Œ (å¦‚ # æˆ– ##)ã€‚
           - è«‹ä½¿ç”¨ Emoji (å¦‚ ğŸ”¥, ğŸš€, ğŸ’¡, ğŸŒŸ) ä¾†å€éš”æ®µè½ã€‚
           - æ¨™é¡Œè«‹å¯«ï¼šâš”ï¸ **Bonus ç„¡é™æŒ‘æˆ°** âš”ï¸
           - **åš´ç¦** ä½¿ç”¨ HTML æ¨™ç±¤ (å¦‚ <br>)ï¼Œè«‹ç›´æ¥æ›è¡Œã€‚
        
        3. **çµæ§‹**ï¼š
           - Part 1: Bonus é¡Œç›®å· (å«é–‹å ´ã€3é¡Œé«˜é›£åº¦é¡Œç›®)ã€‚**ä¸è¦**çµ¦ç­”æ¡ˆã€‚
           - åˆ†éš”ç·š: `|||SEPARATOR|||`
           - Part 2: è§£ç­”å· (å«åƒè€ƒç­”æ¡ˆèˆ‡è§£æ)ã€‚
        """

        try:
            response = model.generate_content(prompt, safety_settings=SAFETY_SETTINGS)
            if response.text and "|||SEPARATOR|||" in response.text:
                parts = response.text.split("|||SEPARATOR|||")
                send_telegram(parts[0].strip())
                user["pending_answers"] = parts[1].strip() 
                
        except Exception as e:
            print(f"Error: {e}")
            send_telegram("âš ï¸ Bonus ç”Ÿæˆå¤±æ•—")

    return user

if __name__ == "__main__":
    v_data, u_data = process_data()
    u_data_updated = run_daily_quiz(v_data, u_data)
    
    save_json(VOCAB_FILE, v_data)
    if u_data_updated:
        save_json(USER_DATA_FILE, u_data_updated)
    else:
        save_json(USER_DATA_FILE, u_data)